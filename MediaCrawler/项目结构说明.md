# 🍁 红枫工具箱 - 项目结构说明

> **版本:** V2.0.0  
> **更新日期:** 2025-11-04  
> **GitHub:** https://github.com/InterestWatcher-Xiaofeng/-

---

## 📂 项目目录结构

```
MediaCrawler/MediaCrawler/
│
├── 📁 config/                          # 配置文件目录
│   ├── __init__.py                    # 配置入口
│   ├── base_config.py                 # 基础配置 ⭐ 核心
│   ├── douyin_config.py               # 抖音配置
│   ├── xhs_config.py                  # 小红书配置
│   ├── bilibili_config.py             # B站配置
│   └── zhihu_config.py                # 知乎配置
│
├── 📁 media_platform/                  # 平台爬虫目录
│   ├── 📁 douyin/                     # 抖音爬虫
│   │   ├── __init__.py
│   │   ├── core.py                    # 核心爬虫逻辑 ⭐
│   │   ├── client.py                  # API客户端
│   │   ├── field.py                   # 字段定义
│   │   └── help.py                    # 辅助函数
│   │
│   ├── 📁 xhs/                        # 小红书爬虫 ⭐ 最新
│   │   ├── __init__.py
│   │   ├── core.py                    # 核心爬虫逻辑
│   │   ├── client.py                  # API客户端(含重试机制)
│   │   ├── field.py                   # 字段定义
│   │   └── help.py                    # 辅助函数
│   │
│   ├── 📁 bilibili/                   # B站爬虫
│   └── 📁 zhihu/                      # 知乎爬虫
│
├── 📁 store/                           # 数据存储目录
│   ├── 📁 douyin/
│   │   └── _store_impl.py             # 抖音存储实现
│   ├── 📁 xhs/
│   │   └── _store_impl.py             # 小红书存储实现
│   ├── 📁 bilibili/
│   └── 📁 zhihu/
│
├── 📁 tools/                           # 工具模块目录
│   ├── async_file_writer.py          # 异步文件写入器 ⭐ 核心
│   ├── utils.py                       # 通用工具函数
│   ├── time_util.py                   # 时间工具
│   └── words.py                       # 词云生成
│
├── 📁 login_data/                      # 登录数据目录
│   ├── 📁 dy_login_info/              # 抖音登录信息
│   │   ├── cookies.json               # Cookie数据
│   │   ├── local_storage.json         # LocalStorage数据
│   │   ├── session_storage.json       # SessionStorage数据
│   │   └── login_info.json            # 登录元信息
│   ├── 📁 xhs_login_info/             # 小红书登录信息
│   ├── 📁 bili_login_info/            # B站登录信息
│   └── 📁 zhihu_login_info/           # 知乎登录信息
│
├── 📁 browser_data/                    # 浏览器数据目录
│   ├── 📁 clean_dy_browser/           # 抖音浏览器数据
│   ├── 📁 clean_xhs_browser/          # 小红书浏览器数据
│   └── ...
│
├── 📁 data/                            # 数据输出目录
│   ├── 📁 douyin/
│   │   └── 📁 csv/                    # CSV文件
│   │       └── 美食_20251104_143052_评论.csv
│   ├── 📁 xhs/
│   │   └── 📁 csv/
│   │       └── 玩具_20251104_150000_评论.csv
│   ├── 📁 bilibili/
│   └── 📁 zhihu/
│
├── 📁 logs/                            # 日志目录
│   └── gui_20251104_143052.log
│
├── 📄 gui_app.py                       # GUI主程序 ⭐⭐⭐ 核心
├── 📄 start_gui.py                     # GUI启动器
├── 📄 统一浏览器采集器.py              # 统一浏览器管理 ⭐⭐⭐ 核心
├── 📄 rpa_xhs_search_crawler.py        # 小红书RPA搜索爬虫 ⭐ 最新
├── 📄 main.py                          # 命令行入口
│
├── 📄 create_icon.py                   # 图标生成脚本
├── 📄 icon.ico                         # 软件图标
├── 📄 icon_preview.png                 # 图标预览
│
├── 📄 项目交接文档.md                  # 完整交接文档 ⭐⭐⭐
├── 📄 项目结构说明.md                  # 本文档
├── 📄 README.md                        # 项目说明
└── 📄 requirements.txt                 # 依赖列表
```

---

## 🔑 核心文件说明

### 1. gui_app.py - GUI主程序 ⭐⭐⭐

**作用:** GUI界面的核心实现

**关键类:** `MediaCrawlerGUI`

**主要功能:**
- 界面布局和组件创建
- 用户交互处理
- 参数配置管理
- 进度显示更新
- 浏览器管理调用

**关键方法:**
```python
class MediaCrawlerGUI:
    def __init__(self):                          # 初始化GUI
    def create_platform_selection(self):         # 创建平台选择界面
    def create_crawler_type_selection(self):     # 创建采集模式选择
    def start_crawling(self):                    # 开始采集(主要逻辑)
    def stop_crawling(self):                     # 停止采集
    def update_progress(self, current, total):   # 更新进度显示
    def create_shared_browser(self, platform):   # 创建统一浏览器
    def perform_login(self, platform):           # 执行登录
```

**代码行数:** 约2500行

---

### 2. 统一浏览器采集器.py - 统一浏览器管理 ⭐⭐⭐

**作用:** 统一管理浏览器实例和采集流程

**关键类:** `UnifiedBrowserCrawler`

**主要功能:**
- 浏览器实例管理
- 登录状态管理
- 平台爬虫调度
- 数据流转控制

**关键方法:**
```python
class UnifiedBrowserCrawler:
    def __init__(self, shared_context, shared_page):  # 初始化
    async def async_douyin_crawler(self, ...):        # 抖音采集
    async def async_xhs_crawler(self, ...):           # 小红书采集 ⭐ 最新
    async def start_unified_douyin_crawling(self):    # 抖音统一浏览器采集
    async def start_unified_xhs_crawling(self):       # 小红书统一浏览器采集
```

**代码行数:** 约650行

---

### 3. rpa_xhs_search_crawler.py - 小红书RPA搜索爬虫 ⭐ 最新

**作用:** 小红书关键词搜索的RPA实现

**关键类:** `RPAXhsSearchCrawler`

**主要功能:**
- RPA搜索关键词
- 设置筛选条件
- 逐个点击笔记卡片
- 获取完整URL(含xsec_token)
- 抓取笔记详情和评论

**关键方法:**
```python
class RPAXhsSearchCrawler:
    def __init__(self, keyword, max_notes):           # 初始化
    async def _goto_search_page(self):                # 访问搜索页
    async def _search_keyword(self):                  # 搜索关键词
    async def _scroll_and_collect_links(self):        # 滚动加载笔记
    async def click_and_scrape_notes(self, client):   # 逐个点击抓取 ⭐ 核心
```

**代码行数:** 约450行

**创建日期:** 2025-11-04

---

### 4. tools/async_file_writer.py - 异步文件写入器 ⭐⭐⭐

**作用:** 管理数据文件的写入和命名

**关键类:** `AsyncFileWriter`

**主要功能:**
- 文件路径生成
- 文件命名规则
- CSV/JSON写入
- 列顺序管理

**关键方法:**
```python
class AsyncFileWriter:
    def _get_file_path(self, file_type, item_type):   # 生成文件路径(核心命名逻辑)
    async def write_to_csv(self, item, item_type):    # 写入CSV文件
    async def write_single_item_to_json(self, item):  # 写入JSON文件
```

**代码行数:** 约300行

---

## 🔄 数据流向

```
用户点击"开始采集"
    ↓
GUI: gui_app.py
    ├─ 更新配置
    ├─ 创建浏览器(如果未创建)
    └─ 调用统一浏览器采集器
    ↓
统一浏览器采集器: 统一浏览器采集器.py
    ├─ 检查浏览器状态
    ├─ 创建平台客户端
    └─ 调用平台爬虫
    ↓
平台爬虫: media_platform/{platform}/core.py
    ├─ 搜索关键词(或打开链接)
    ├─ 获取内容列表
    ├─ 遍历内容
    │   ├─ 获取详情
    │   └─ 获取评论
    └─ 保存数据
    ↓
数据存储: store/{platform}/_store_impl.py
    ├─ 选择存储方式(CSV/JSON)
    └─ 调用AsyncFileWriter
    ↓
文件写入: tools/async_file_writer.py
    ├─ 生成文件路径
    ├─ 写入数据
    └─ 关闭文件
    ↓
输出文件: data/{platform}/csv/
    └─ 关键词_时间戳_评论.csv
```

---

## 🎯 关键配置文件

### config/base_config.py - 基础配置

**核心参数:**
```python
# 平台配置
PLATFORM = "xhs"                        # 平台: xhs | dy | bili | zhihu
KEYWORDS = "玩具"                       # 搜索关键词
CRAWLER_TYPE = "search"                 # 采集模式: search | detail | creator

# 采集控制
CRAWLER_MAX_NOTES_COUNT = 3             # 每个关键词采集的笔记数
CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = 50  # 每个笔记采集的评论数
ENABLE_GET_SUB_COMMENTS = False         # 是否采集二级评论

# 数据保存
SAVE_DATA_OPTION = "csv"                # 保存格式: csv | json | db
ENABLE_GET_COMMENTS = True              # 是否采集评论
ENABLE_GET_CONTENTS = False             # 是否采集内容(已禁用)

# 浏览器配置
HEADLESS = False                        # 无头模式
SAVE_LOGIN_STATE = True                 # 保存登录状态
```

---

## 📊 文件命名规则

### 关键词搜索模式
```
格式: {关键词}_{时间戳}_{类型}.{格式}
示例: 玩具_20251104_143052_评论.csv
```

### 多链接模式
```
格式: {时间戳}_{数量}条笔记_{类型}.{格式}
示例: 20251104_143052_5条笔记_评论.csv
```

### 创作者模式
```
格式: {创作者昵称}_{数量}条笔记_{类型}.{格式}
示例: 美食博主_10条笔记_评论.csv
```

---

## 🔐 登录数据结构

### login_data/{platform}_login_info/

**cookies.json** - Cookie数据
```json
[
    {
        "name": "sessionid",
        "value": "xxx",
        "domain": ".xiaohongshu.com",
        "path": "/",
        "expires": 1234567890
    }
]
```

**local_storage.json** - LocalStorage数据
```json
{
    "key1": "value1",
    "key2": "value2"
}
```

**login_info.json** - 登录元信息
```json
{
    "platform": "xhs",
    "login_time": "2025-11-04 14:30:52",
    "cookies_count": 13,
    "local_storage_count": 21,
    "session_storage_count": 2
}
```

---

## 📝 日志文件

### logs/gui_YYYYMMDD_HHMMSS.log

**日志级别:**
- `[INFO]` - 正常信息
- `[WARNING]` - 警告(不影响功能)
- `[ERROR]` - 错误(影响功能)
- `[DEBUG]` - 调试信息(默认不显示)

**关键日志标识:**
```
[INFO] MediaCrawler GUI 启动
[INFO] ✅ 登录状态有效
[INFO] 🚀 开始采集关键词: 玩具
[INFO] 🔥 正在采集第1个笔记: xxx
[INFO] ✅ 采集完成
```

---

## 🚀 快速定位问题

### 问题1: 登录失败
**查看文件:** `logs/gui_YYYYMMDD_HHMMSS.log`  
**关键字:** `登录失败`, `Cookie`, `ERROR`

### 问题2: 采集无数据
**查看文件:** `logs/gui_YYYYMMDD_HHMMSS.log`  
**关键字:** `采集失败`, `未找到`, `ERROR`

### 问题3: 文件命名错误
**查看文件:** `tools/async_file_writer.py`  
**方法:** `_get_file_path()`

### 问题4: 配置不生效
**查看文件:** `config/base_config.py`  
**检查:** GUI是否正确更新配置

---

**文档结束**

*更新日期: 2025-11-04*
*版本: V2.0.0*
*作者: yufeng*

