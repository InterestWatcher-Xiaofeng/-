#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å™¨
è§£å†³GUIç™»å½•å’Œçˆ¬è™«é‡‡é›†ä½¿ç”¨ä¸åŒæµè§ˆå™¨å®ä¾‹çš„é—®é¢˜
ä½¿ç”¨åŒä¸€ä¸ªæµè§ˆå™¨çª—å£è¿›è¡Œç™»å½•å’Œåç»­æ•°æ®é‡‡é›†
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

import config
from media_platform.douyin.core import DouYinCrawler
from tools.utils import utils

class UnifiedBrowserCrawler:
    """ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å™¨"""

    def __init__(self, shared_context=None, shared_page=None, progress_callback=None, stop_flag_callback=None, gui_app=None):
        """
        åˆå§‹åŒ–ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å™¨

        Args:
            shared_context: GUIæä¾›çš„å…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡
            shared_page: GUIæä¾›çš„å…±äº«é¡µé¢
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, message)
            stop_flag_callback: åœæ­¢æ ‡å¿—æ£€æŸ¥å‡½æ•° callback() -> bool
            gui_app: GUIåº”ç”¨å®ä¾‹ï¼Œç”¨äºè®¾ç½®éªŒè¯çŠ¶æ€
        """
        self.shared_context = shared_context
        self.shared_page = shared_page
        self.crawler = None
        self.progress_callback = progress_callback
        self.stop_flag_callback = stop_flag_callback
        self.gui_app = gui_app  # ğŸ”¥ æ–°å¢ï¼šGUIåº”ç”¨å®ä¾‹

    def should_stop(self):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢é‡‡é›†"""
        if self.stop_flag_callback:
            return self.stop_flag_callback()
        return False
        
    async def setup_crawler(self, platform: str = "dy"):
        """è®¾ç½®çˆ¬è™«å®ä¾‹"""
        if platform == "dy":
            self.crawler = DouYinCrawler()
            # ğŸ”¥ å…³é”®ï¼šå°†å…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡æ³¨å…¥åˆ°çˆ¬è™«ä¸­
            if hasattr(self.crawler, 'browser_context'):
                self.crawler.browser_context = self.shared_context
            if hasattr(self.crawler, 'context'):
                self.crawler.context = self.shared_context
        elif platform == "xhs":
            from media_platform.xhs import XiaoHongShuCrawler
            self.crawler = XiaoHongShuCrawler()
            # ğŸ”¥ å…³é”®ï¼šå°†å…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡æ³¨å…¥åˆ°çˆ¬è™«ä¸­
            if hasattr(self.crawler, 'browser_context'):
                self.crawler.browser_context = self.shared_context
            if hasattr(self.crawler, 'context'):
                self.crawler.context = self.shared_context
        else:
            raise ValueError(f"æš‚ä¸æ”¯æŒå¹³å°: {platform}")
    
    async def start_search_crawling(self, keywords: str, max_count: int = 20,
                                    max_comments_per_video: int = 50,
                                    enable_comments: bool = True,
                                    enable_sub_comments: bool = True,
                                    save_format: str = "csv",
                                    output_dir: str = None,
                                    platform: str = "dy"):
        """
        å¼€å§‹æœç´¢é‡‡é›†

        Args:
            keywords: æœç´¢å…³é”®è¯
            max_count: æœ€å¤§é‡‡é›†æ•°é‡ï¼ˆè§†é¢‘/ç¬”è®°æ•°é‡ï¼‰
            max_comments_per_video: æ¯ä¸ªè§†é¢‘/ç¬”è®°æœ€å¤§è¯„è®ºæ•°é‡
            enable_comments: æ˜¯å¦é‡‡é›†ä¸€çº§è¯„è®º
            enable_sub_comments: æ˜¯å¦é‡‡é›†äºŒçº§è¯„è®º
            save_format: ä¿å­˜æ ¼å¼ (csv/json/sqlite/db)
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç›®å½•ï¼‰
            platform: å¹³å° (dy/xhs)
        """
        try:
            # ğŸ”¥ è®¾ç½®å®Œæ•´é…ç½®
            config.KEYWORDS = keywords
            config.CRAWLER_MAX_NOTES_COUNT = max_count
            config.CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = max_comments_per_video
            config.CRAWLER_TYPE = "search"
            config.PLATFORM = platform  # ğŸ”¥ ä½¿ç”¨ä¼ å…¥çš„å¹³å°å‚æ•°
            config.ENABLE_GET_COMMENTS = enable_comments
            config.ENABLE_GET_SUB_COMMENTS = enable_sub_comments
            config.SAVE_DATA_OPTION = save_format
            config.ENABLE_RPA_SEARCH = True  # ğŸ”¥ å¯ç”¨RPAæœç´¢æ¨¡å¼

            # ğŸ”¥ æ¯æ¬¡é‡‡é›†å‰é‡ç½®storeå®ä¾‹å’Œç¼“å­˜
            if platform == "dy":
                from store.douyin import DouyinStoreFactory
                import store.douyin as douyin_store
                DouyinStoreFactory.reset_store()
                douyin_store._video_info_cache.clear()
                if output_dir:
                    DouyinStoreFactory.set_output_dir(output_dir)
                content_type = "è§†é¢‘"
            elif platform == "xhs":
                from store.xhs import XhsStoreFactory
                import store.xhs as xhs_store
                XhsStoreFactory.reset_store()
                if hasattr(xhs_store, '_note_info_cache'):
                    xhs_store._note_info_cache.clear()
                if output_dir:
                    XhsStoreFactory.set_output_dir(output_dir)
                content_type = "ç¬”è®°"
            else:
                content_type = "å†…å®¹"

            print(f"ğŸš€ å¼€å§‹ç»Ÿä¸€æµè§ˆå™¨é‡‡é›† - {platform.upper()}")
            print(f"ğŸ” å…³é”®è¯: {keywords}")
            print(f"ğŸ“Š {content_type}æ•°é‡: {max_count} ä¸ª")
            print(f"ğŸ’¬ æ¯ä¸ª{content_type}è¯„è®ºæ•°: {max_comments_per_video} æ¡")
            print(f"âœ… ä¸€çº§è¯„è®º: {enable_comments}")
            print(f"âœ… äºŒçº§è¯„è®º: {enable_sub_comments}")
            print(f"ğŸ’¾ ä¿å­˜æ ¼å¼: {save_format}")
            print(f"ğŸ”¥ ä½¿ç”¨å…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡")

            # è®¾ç½®çˆ¬è™«
            await self.setup_crawler(platform)

            # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨ç»Ÿä¸€æµè§ˆå™¨è¿›è¡Œé‡‡é›†
            if self.crawler:
                if platform == "dy":
                    await self.start_unified_douyin_crawling()
                elif platform == "xhs":
                    await self.start_unified_xiaohongshu_crawling()

            print(f"âœ… é‡‡é›†å®Œæˆï¼")

            # ğŸ”¥ è¿”å›ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„ï¼ˆä¼ é€’å…³é”®è¯ç”¨äºæ–‡ä»¶å‘½åï¼‰
            return self._get_generated_files(save_format, output_dir, keywords, platform)

        except Exception as e:
            print(f"âŒ é‡‡é›†å¤±è´¥: {str(e)}")
            raise

    def _get_generated_files(self, save_format: str, output_dir: str = None, keywords: str = "", platform: str = "dy") -> dict:
        """
        è·å–ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„

        ğŸ”¥ æ–°å‘½åè§„åˆ™ï¼šå¹³å°_å…³é”®è¯.æ ¼å¼
        ç¤ºä¾‹ï¼šæŠ–éŸ³_ç¾é£Ÿæ¢åº—.csv, å°çº¢ä¹¦_ç¾é£Ÿæ¢åº—.csv
        """
        import os
        import re

        # ğŸ”¥ æ ¹æ®å¹³å°è®¾ç½®é»˜è®¤è·¯å¾„å’Œå¹³å°åç§°
        platform_names = {
            "dy": "æŠ–éŸ³",
            "xhs": "å°çº¢ä¹¦",
            "bili": "Bç«™",
            "ks": "å¿«æ‰‹",
            "wb": "å¾®åš",
            "tieba": "è´´å§",
            "zhihu": "çŸ¥ä¹"
        }
        platform_name = platform_names.get(platform, platform.upper())

        if output_dir:
            base_path = output_dir
        else:
            platform_dirs = {
                "dy": "douyin",
                "xhs": "xhs",
                "bili": "bilibili",
                "ks": "kuaishou",
                "wb": "weibo",
                "tieba": "tieba",
                "zhihu": "zhihu"
            }
            platform_dir = platform_dirs.get(platform, platform)
            base_path = f"data/{platform_dir}/{save_format}"

        # ğŸ”¥ æ¸…ç†å…³é”®è¯ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        clean_keywords = re.sub(r'[\\/:*?"<>|\s]+', '_', keywords.strip())
        if not clean_keywords:
            clean_keywords = "æœªå‘½å"

        # ğŸ”¥ æ–°å‘½åæ ¼å¼ï¼šå¹³å°_å…³é”®è¯
        files = {
            "contents": f"{base_path}/{platform_name}_{clean_keywords}_å†…å®¹.{save_format}",
            "comments": f"{base_path}/{platform_name}_{clean_keywords}_è¯„è®º.{save_format}"
        }

        # åªè¿”å›å­˜åœ¨çš„æ–‡ä»¶
        existing_files = {}
        for key, path in files.items():
            if os.path.exists(path):
                existing_files[key] = path
                print(f"ğŸ“„ {key}æ–‡ä»¶: {path}")

        return existing_files

    async def start_detail_crawling(self, video_url: str,
                                    max_comments_per_video: int = 50,
                                    enable_comments: bool = True,
                                    enable_sub_comments: bool = True,
                                    save_format: str = "csv",
                                    output_dir: str = None):
        """
        å¼€å§‹é“¾æ¥é‡‡é›† (Detailæ¨¡å¼)

        Args:
            video_url: è§†é¢‘é“¾æ¥æˆ–ID
            max_comments_per_video: æ¯ä¸ªè§†é¢‘æœ€å¤§è¯„è®ºæ•°é‡
            enable_comments: æ˜¯å¦é‡‡é›†ä¸€çº§è¯„è®º
            enable_sub_comments: æ˜¯å¦é‡‡é›†äºŒçº§è¯„è®º
            save_format: ä¿å­˜æ ¼å¼ (csv/json/sqlite/db)
            output_dir: è¾“å‡ºç›®å½•
        """
        try:
            print(f"\n{'='*60}")
            print(f"ğŸš€ å¼€å§‹é“¾æ¥é‡‡é›†")
            print(f"{'='*60}")
            print(f"ğŸ”— è§†é¢‘é“¾æ¥: {video_url}")
            print(f"ğŸ’¬ è¯„è®ºæ•°: {max_comments_per_video} æ¡")
            print(f"ğŸ’¾ ä¿å­˜æ ¼å¼: {save_format}")
            print(f"{'='*60}\n")

            # ğŸ”¥ éªŒè¯é“¾æ¥æ ¼å¼
            from media_platform.douyin.help import parse_video_info_from_url
            try:
                video_info = parse_video_info_from_url(video_url)
                print(f"âœ… é“¾æ¥è§£ææˆåŠŸ:")
                print(f"   è§†é¢‘ID: {video_info.aweme_id}")
                print(f"   é“¾æ¥ç±»å‹: {video_info.url_type}")
            except Exception as parse_error:
                print(f"âŒ é“¾æ¥è§£æå¤±è´¥: {parse_error}")
                print(f"   è¯·æ£€æŸ¥é“¾æ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
                print(f"   æ”¯æŒçš„æ ¼å¼:")
                print(f"   1. å®Œæ•´é“¾æ¥: https://www.douyin.com/video/7525538910311632128")
                print(f"   2. çŸ­é“¾æ¥: https://v.douyin.com/drIPtQ_WPWY/")
                print(f"   3. çº¯ID: 7525538910311632128")
                raise

            # ğŸ”¥ è®¾ç½®é…ç½®
            config.DY_SPECIFIED_ID_LIST = [video_url]  # å•ä¸ªé“¾æ¥
            config.CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = max_comments_per_video
            config.CRAWLER_TYPE = "detail"
            config.PLATFORM = "dy"
            config.ENABLE_GET_COMMENTS = enable_comments
            config.ENABLE_GET_SUB_COMMENTS = enable_sub_comments
            config.SAVE_DATA_OPTION = save_format

            # ğŸ”¥ é‡ç½®store
            from store.douyin import DouyinStoreFactory
            import store.douyin as douyin_store
            DouyinStoreFactory.reset_store()
            douyin_store._video_info_cache.clear()

            if output_dir:
                DouyinStoreFactory.set_output_dir(output_dir)

            # è®¾ç½®çˆ¬è™«
            await self.setup_crawler("dy")

            # å¼€å§‹é‡‡é›†
            if self.crawler:
                await self.start_unified_douyin_crawling()

            print(f"\nâœ… é“¾æ¥é‡‡é›†å®Œæˆï¼\n")

            # è¿”å›ç”Ÿæˆçš„æ–‡ä»¶
            return self._get_generated_files(save_format, output_dir, video_url)

        except Exception as e:
            print(f"\nâŒ é“¾æ¥é‡‡é›†å¤±è´¥: {str(e)}\n")
            import traceback
            traceback.print_exc()
            raise

    async def start_creator_crawling(self, creator_url: str, max_count: int = 20,
                                     max_comments_per_video: int = 50,
                                     enable_comments: bool = True,
                                     enable_sub_comments: bool = True,
                                     save_format: str = "csv",
                                     output_dir: str = None):
        """
        å¼€å§‹åˆ›ä½œè€…é‡‡é›† (Creatoræ¨¡å¼)

        Args:
            creator_url: åˆ›ä½œè€…é“¾æ¥æˆ–ID
            max_count: æœ€å¤§é‡‡é›†è§†é¢‘æ•°é‡
            max_comments_per_video: æ¯ä¸ªè§†é¢‘æœ€å¤§è¯„è®ºæ•°é‡
            enable_comments: æ˜¯å¦é‡‡é›†ä¸€çº§è¯„è®º
            enable_sub_comments: æ˜¯å¦é‡‡é›†äºŒçº§è¯„è®º
            save_format: ä¿å­˜æ ¼å¼ (csv/json/sqlite/db)
            output_dir: è¾“å‡ºç›®å½•
        """
        try:
            # ğŸ”¥ è®¾ç½®é…ç½®
            config.DY_CREATOR_ID_LIST = [creator_url]  # å•ä¸ªåˆ›ä½œè€…
            config.CRAWLER_MAX_NOTES_COUNT = max_count
            config.CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = max_comments_per_video
            config.CRAWLER_TYPE = "creator"
            config.PLATFORM = "dy"
            config.ENABLE_GET_COMMENTS = enable_comments
            config.ENABLE_GET_SUB_COMMENTS = enable_sub_comments
            config.SAVE_DATA_OPTION = save_format

            # ğŸ”¥ é‡ç½®store
            from store.douyin import DouyinStoreFactory
            import store.douyin as douyin_store
            DouyinStoreFactory.reset_store()
            douyin_store._video_info_cache.clear()

            if output_dir:
                DouyinStoreFactory.set_output_dir(output_dir)

            print(f"ğŸš€ å¼€å§‹åˆ›ä½œè€…é‡‡é›†")
            print(f"ğŸ‘¤ åˆ›ä½œè€…: {creator_url}")
            print(f"ğŸ“Š è§†é¢‘æ•°é‡: {max_count} ä¸ª")
            print(f"ğŸ’¬ æ¯ä¸ªè§†é¢‘è¯„è®ºæ•°: {max_comments_per_video} æ¡")
            print(f"ğŸ’¾ ä¿å­˜æ ¼å¼: {save_format}")

            # è®¾ç½®çˆ¬è™«
            await self.setup_crawler("dy")

            # å¼€å§‹é‡‡é›†
            if self.crawler:
                await self.start_unified_douyin_crawling()

            print(f"âœ… åˆ›ä½œè€…é‡‡é›†å®Œæˆï¼")

            # è¿”å›ç”Ÿæˆçš„æ–‡ä»¶
            return self._get_generated_files(save_format, output_dir, creator_url)

        except Exception as e:
            print(f"âŒ åˆ›ä½œè€…é‡‡é›†å¤±è´¥: {str(e)}")
            raise

    async def start_unified_douyin_crawling(self):
        """ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€æµè§ˆå™¨è¿›è¡ŒæŠ–éŸ³é‡‡é›† - æ”¯æŒä¸‰ç§æ¨¡å¼"""
        try:
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ‡è®°è¿™æ˜¯ç»Ÿä¸€æµè§ˆå™¨æ¨¡å¼ï¼Œä¸è¦å…³é—­æµè§ˆå™¨ä¸Šä¸‹æ–‡
            self.crawler._is_unified_browser = True

            # ç›´æ¥è®¾ç½®æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡æµè§ˆå™¨å¯åŠ¨
            self.crawler.browser_context = self.shared_context
            self.crawler.context_page = self.shared_page

            # ğŸ”¥ ä¼ é€’è¿›åº¦å›è°ƒç»™çˆ¬è™«
            if self.progress_callback:
                self.crawler.progress_callback = self.progress_callback

            # ğŸ”¥ æ¯æ¬¡é‡‡é›†éƒ½é‡æ–°åˆ›å»ºæŠ–éŸ³å®¢æˆ·ç«¯ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„cookies
            from media_platform.douyin.client import DouYinClient
            print("ğŸ”„ åˆ›å»ºæ–°çš„æŠ–éŸ³å®¢æˆ·ç«¯...")
            self.crawler.dy_client = await self.crawler.create_douyin_client(None)

            # æ£€æŸ¥ç™»å½•çŠ¶æ€
            print("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
            if not await self.crawler.dy_client.pong(browser_context=self.shared_context):
                print("âš ï¸ ç™»å½•çŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œä½†ç»§ç»­å°è¯•é‡‡é›†...")

                # ğŸ”¥ è®¾ç½®éªŒè¯çŠ¶æ€æ ‡å¿—
                if self.gui_app:
                    self.gui_app.is_verifying = True

                # ğŸ”¥ å¼¹çª—æç¤ºç”¨æˆ·å¯èƒ½éœ€è¦éªŒè¯
                import tkinter.messagebox as messagebox
                import threading
                def show_warning():
                    messagebox.showwarning(
                        "ç™»å½•éªŒè¯æç¤º",
                        "æ£€æµ‹åˆ°å¯èƒ½éœ€è¦ç™»å½•éªŒè¯ï¼\n\n"
                        "è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆä»¥ä¸‹æ“ä½œï¼š\n"
                        "1. æ‰«ç ç™»å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰\n"
                        "2. å®Œæˆæ‰‹æœºå·éªŒè¯ï¼ˆå¦‚æœéœ€è¦ï¼‰\n"
                        "3. å®Œæˆæ»‘åŠ¨éªŒè¯ç ï¼ˆå¦‚æœéœ€è¦ï¼‰\n\n"
                        "éªŒè¯å®Œæˆåï¼Œé‡‡é›†å°†è‡ªåŠ¨ç»§ç»­ã€‚\n"
                        "æ­¤è¿‡ç¨‹ä¸­ç‚¹å‡»ã€åœæ­¢é‡‡é›†ã€‘æŒ‰é’®æ— æ•ˆã€‚"
                    )
                # åœ¨æ–°çº¿ç¨‹ä¸­æ˜¾ç¤ºå¼¹çª—ï¼Œé¿å…é˜»å¡
                threading.Thread(target=show_warning, daemon=True).start()
                # ç­‰å¾…60ç§’ï¼Œç»™ç”¨æˆ·æ—¶é—´éªŒè¯
                print("â³ ç­‰å¾…60ç§’ï¼Œç»™ç”¨æˆ·æ—¶é—´å®ŒæˆéªŒè¯...")
                await asyncio.sleep(60)

                # ğŸ”¥ éªŒè¯å®Œæˆï¼Œæ¸…é™¤éªŒè¯çŠ¶æ€æ ‡å¿—
                if self.gui_app:
                    self.gui_app.is_verifying = False

            # æ›´æ–°å®¢æˆ·ç«¯cookies
            print("ğŸª æ›´æ–°å®¢æˆ·ç«¯cookies...")
            await self.crawler.dy_client.update_cookies(browser_context=self.shared_context)

            # ğŸ”¥ æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒçš„é‡‡é›†
            from var import crawler_type_var
            crawler_type_var.set(config.CRAWLER_TYPE)

            if config.CRAWLER_TYPE == "search":
                # ğŸ”¥ ä½¿ç”¨RPAæœç´¢æ¨¡å¼
                if config.ENABLE_RPA_SEARCH:
                    print("ğŸ” å¼€å§‹RPAæœç´¢é‡‡é›†...")
                    await self._rpa_search_and_collect()
                else:
                    print("ğŸ” å¼€å§‹APIæœç´¢é‡‡é›†...")
                    await self.crawler.search()
            elif config.CRAWLER_TYPE == "detail":
                print("ğŸ”— å¼€å§‹é“¾æ¥é‡‡é›†...")
                await self.crawler.get_specified_awemes()
            elif config.CRAWLER_TYPE == "creator":
                print("ğŸ‘¤ å¼€å§‹åˆ›ä½œè€…é‡‡é›†...")
                await self.crawler.get_creators_and_videos()
            else:
                raise ValueError(f"æœªçŸ¥çš„é‡‡é›†æ¨¡å¼: {config.CRAWLER_TYPE}")

            print(f"âœ… {config.CRAWLER_TYPE}é‡‡é›†å®Œæˆ")

        except Exception as e:
            print(f"âŒ ç»Ÿä¸€æµè§ˆå™¨æŠ–éŸ³é‡‡é›†å¤±è´¥: {e}")
            print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"   é”™è¯¯è¯¦æƒ…: {repr(e)}")
            import traceback
            print(f"   å®Œæ•´å †æ ˆ:")
            traceback.print_exc()
            raise Exception(f"ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å¤±è´¥: {type(e).__name__}: {str(e)}") from e

    async def start_unified_xiaohongshu_crawling(self):
        """ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€æµè§ˆå™¨è¿›è¡Œå°çº¢ä¹¦é‡‡é›† - æ”¯æŒä¸‰ç§æ¨¡å¼"""
        try:
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ‡è®°è¿™æ˜¯ç»Ÿä¸€æµè§ˆå™¨æ¨¡å¼ï¼Œä¸è¦å…³é—­æµè§ˆå™¨ä¸Šä¸‹æ–‡
            self.crawler._is_unified_browser = True

            # ç›´æ¥è®¾ç½®æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡æµè§ˆå™¨å¯åŠ¨
            self.crawler.browser_context = self.shared_context
            self.crawler.context_page = self.shared_page

            # ğŸ”¥ ä¼ é€’è¿›åº¦å›è°ƒç»™çˆ¬è™«
            if self.progress_callback:
                self.crawler.progress_callback = self.progress_callback

            # ğŸ”¥ æ¯æ¬¡é‡‡é›†éƒ½é‡æ–°åˆ›å»ºå°çº¢ä¹¦å®¢æˆ·ç«¯ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„cookies
            from media_platform.xhs.client import XiaoHongShuClient
            print("ğŸ”„ åˆ›å»ºæ–°çš„å°çº¢ä¹¦å®¢æˆ·ç«¯...")
            self.crawler.xhs_client = await self.crawler.create_xhs_client(None)

            # æ£€æŸ¥ç™»å½•çŠ¶æ€
            print("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
            if not await self.crawler.xhs_client.pong():
                print("âš ï¸ ç™»å½•çŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œä½†ç»§ç»­å°è¯•é‡‡é›†...")

                # ğŸ”¥ è®¾ç½®éªŒè¯çŠ¶æ€æ ‡å¿—
                if self.gui_app:
                    self.gui_app.is_verifying = True

                # ğŸ”¥ å¼¹çª—æç¤ºç”¨æˆ·å¯èƒ½éœ€è¦éªŒè¯
                import tkinter.messagebox as messagebox
                import threading
                def show_warning():
                    messagebox.showwarning(
                        "ç™»å½•éªŒè¯æç¤º",
                        "æ£€æµ‹åˆ°å¯èƒ½éœ€è¦ç™»å½•éªŒè¯ï¼\n\n"
                        "è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆä»¥ä¸‹æ“ä½œï¼š\n"
                        "1. æ‰«ç ç™»å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰\n"
                        "2. å®Œæˆæ»‘åŠ¨éªŒè¯ç ï¼ˆå¦‚æœéœ€è¦ï¼‰\n\n"
                        "éªŒè¯å®Œæˆåï¼Œé‡‡é›†å°†è‡ªåŠ¨ç»§ç»­ã€‚\n"
                        "æ­¤è¿‡ç¨‹ä¸­ç‚¹å‡»ã€åœæ­¢é‡‡é›†ã€‘æŒ‰é’®æ— æ•ˆã€‚"
                    )
                # åœ¨æ–°çº¿ç¨‹ä¸­æ˜¾ç¤ºå¼¹çª—ï¼Œé¿å…é˜»å¡
                threading.Thread(target=show_warning, daemon=True).start()
                # ç­‰å¾…60ç§’ï¼Œç»™ç”¨æˆ·æ—¶é—´éªŒè¯
                print("â³ ç­‰å¾…60ç§’ï¼Œç»™ç”¨æˆ·æ—¶é—´å®ŒæˆéªŒè¯...")
                await asyncio.sleep(60)

                # ğŸ”¥ éªŒè¯å®Œæˆï¼Œæ¸…é™¤éªŒè¯çŠ¶æ€æ ‡å¿—
                if self.gui_app:
                    self.gui_app.is_verifying = False

            # æ›´æ–°å®¢æˆ·ç«¯cookies
            print("ğŸª æ›´æ–°å®¢æˆ·ç«¯cookies...")
            await self.crawler.xhs_client.update_cookies(browser_context=self.shared_context)

            # ğŸ”¥ æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒçš„é‡‡é›†
            from var import crawler_type_var
            crawler_type_var.set(config.CRAWLER_TYPE)

            if config.CRAWLER_TYPE == "search":
                # ğŸ”¥ ä½¿ç”¨RPAæœç´¢æ¨¡å¼
                if config.ENABLE_RPA_SEARCH:
                    print("ğŸ” å¼€å§‹RPAæœç´¢é‡‡é›†...")
                    await self._rpa_search_and_collect()
                else:
                    print("ğŸ” å¼€å§‹APIæœç´¢é‡‡é›†...")
                    await self.crawler.search()
            elif config.CRAWLER_TYPE == "detail":
                print("ğŸ”— å¼€å§‹é“¾æ¥é‡‡é›†...")
                await self.crawler.get_specified_notes()
            elif config.CRAWLER_TYPE == "creator":
                print("ğŸ‘¤ å¼€å§‹åˆ›ä½œè€…é‡‡é›†...")
                await self.crawler.get_creators_and_notes()
            else:
                raise ValueError(f"æœªçŸ¥çš„é‡‡é›†æ¨¡å¼: {config.CRAWLER_TYPE}")

            print(f"âœ… {config.CRAWLER_TYPE}é‡‡é›†å®Œæˆ")

        except Exception as e:
            print(f"âŒ ç»Ÿä¸€æµè§ˆå™¨å°çº¢ä¹¦é‡‡é›†å¤±è´¥: {e}")
            print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"   é”™è¯¯è¯¦æƒ…: {repr(e)}")
            import traceback
            print(f"   å®Œæ•´å †æ ˆ:")
            traceback.print_exc()
            raise Exception(f"ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å¤±è´¥: {type(e).__name__}: {str(e)}") from e

    async def _rpa_search_and_collect(self):
        """ğŸ”¥ RPAæœç´¢å¹¶æ”¶é›†é“¾æ¥,ç„¶åæŠ“å–è¯„è®º - æ”¯æŒæŠ–éŸ³å’Œå°çº¢ä¹¦"""
        import asyncio

        # ğŸ”¥ æ ¹æ®å¹³å°é€‰æ‹©ä¸åŒçš„RPAçˆ¬è™«
        if config.PLATFORM == "dy":
            from rpa_search_crawler import RPASearchCrawler

            # è·å–å…³é”®è¯
            keyword = config.KEYWORDS.split(',')[0].strip()
            max_count = config.CRAWLER_MAX_NOTES_COUNT

            print(f"ğŸ¯ æŠ–éŸ³RPAæœç´¢å‚æ•°:")
            print(f"   å…³é”®è¯: {keyword}")
            print(f"   è§†é¢‘æ•°é‡: {max_count}")

            # åˆ›å»ºRPAæœç´¢çˆ¬è™«(ä½¿ç”¨å…±äº«æµè§ˆå™¨)
            rpa_crawler = RPASearchCrawler(keyword=keyword, max_videos=max_count)

            # ğŸ”¥ å…³é”®:ä½¿ç”¨å…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡
            rpa_crawler.context = self.shared_context
            rpa_crawler.page = self.shared_page

            # æ‰§è¡ŒRPAæœç´¢(è·³è¿‡æµè§ˆå™¨å¯åŠ¨å’Œç™»å½•)
            print("ğŸ” å¼€å§‹RPAæœç´¢...")
            await rpa_crawler._goto_search_page()
            await rpa_crawler._search_keyword()
            await rpa_crawler._scroll_and_collect_links()

            links = rpa_crawler.video_links
            print(f"âœ… RPAæœç´¢å®Œæˆ,æ”¶é›†åˆ° {len(links)} ä¸ªè§†é¢‘é“¾æ¥")

            # ğŸ”¥ å°†é“¾æ¥è®¾ç½®åˆ°é…ç½®,ç„¶åè°ƒç”¨detailæ¨¡å¼æŠ“å–
            if links:
                config.DY_SPECIFIED_ID_LIST = links
                config.CRAWLER_TYPE = "detail"  # åˆ‡æ¢åˆ°detailæ¨¡å¼
                # ğŸ”¥ æ³¨æ„:CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTESå·²ç»åœ¨start_search_crawling()ä¸­è®¾ç½®äº†

                print("ğŸ”— å¼€å§‹æŠ“å–è§†é¢‘è¯„è®º...")
                print(f"   æ¯ä¸ªè§†é¢‘è¯„è®ºæ•°: {config.CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES} æ¡")
                await self.crawler.get_specified_awemes()
            else:
                print("âš ï¸ æœªæ”¶é›†åˆ°è§†é¢‘é“¾æ¥,è·³è¿‡è¯„è®ºæŠ“å–")

        elif config.PLATFORM == "xhs":
            from rpa_xhs_search_crawler import RPAXhsSearchCrawler

            # è·å–å…³é”®è¯
            keyword = config.KEYWORDS.split(',')[0].strip()
            max_count = config.CRAWLER_MAX_NOTES_COUNT

            print(f"ğŸ¯ å°çº¢ä¹¦RPAæœç´¢å‚æ•°:")
            print(f"   å…³é”®è¯: {keyword}")
            print(f"   ç¬”è®°æ•°é‡: {max_count}")

            # åˆ›å»ºRPAæœç´¢çˆ¬è™«(ä½¿ç”¨å…±äº«æµè§ˆå™¨)
            rpa_crawler = RPAXhsSearchCrawler(keyword=keyword, max_notes=max_count)

            # ğŸ”¥ å…³é”®:ä½¿ç”¨å…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡
            rpa_crawler.context = self.shared_context
            rpa_crawler.page = self.shared_page

            # æ‰§è¡ŒRPAæœç´¢(è·³è¿‡æµè§ˆå™¨å¯åŠ¨å’Œç™»å½•)
            print("ğŸ” å¼€å§‹RPAæœç´¢...")
            await rpa_crawler._goto_search_page()
            await rpa_crawler._search_keyword()
            await rpa_crawler._scroll_and_collect_links()

            # ğŸ”¥ æ–°æµç¨‹: ç›´æ¥åœ¨RPAä¸­é€ä¸ªç‚¹å‡»ç¬”è®°å¹¶æŠ“å–è¯„è®º
            if hasattr(rpa_crawler, 'ready_to_click') and rpa_crawler.ready_to_click:
                print(f"\nğŸ”¥ å¼€å§‹æ¨¡æ‹ŸçœŸå®ç”¨æˆ·: ä»å·¦åˆ°å³ä¾æ¬¡ç‚¹å‡»ç¬”è®°å¹¶æŠ“å–è¯„è®º")

                # è·å–å°çº¢ä¹¦å®¢æˆ·ç«¯
                xhs_client = self.crawler.xhs_client

                # ğŸ”¥ è°ƒç”¨æ–°æ–¹æ³•: é€ä¸ªç‚¹å‡»ç¬”è®°å¹¶ç›´æ¥æŠ“å–è¯„è®º
                all_notes_data = await rpa_crawler.click_and_scrape_notes(xhs_client)

                # ğŸ”¥ å°†æ•°æ®ä¿å­˜åˆ°crawlerä¸­
                if all_notes_data:
                    print(f"\nâœ… æˆåŠŸæŠ“å– {len(all_notes_data)} ä¸ªç¬”è®°çš„æ•°æ®")

                    # å°†æ•°æ®è½¬æ¢ä¸ºcrawleréœ€è¦çš„æ ¼å¼å¹¶ä¿å­˜
                    for note_data in all_notes_data:
                        note_detail = note_data.get('note_detail')
                        comments = note_data.get('comments', [])

                        if note_detail:
                            # ä¿å­˜ç¬”è®°è¯¦æƒ…
                            await self.crawler.xhs_store.store_content(note_detail)

                            # ä¿å­˜è¯„è®º
                            if comments:
                                await self.crawler.batch_update_note_comments(note_detail['note_id'], comments)

                    print(f"âœ… æ‰€æœ‰æ•°æ®å·²ä¿å­˜")
                else:
                    print("âš ï¸ æœªæˆåŠŸæŠ“å–åˆ°æ•°æ®")
            else:
                print("âš ï¸ é¡µé¢æœªå‡†å¤‡å¥½,è·³è¿‡æŠ“å–")
        else:
            print(f"âš ï¸ å¹³å° {config.PLATFORM} æš‚ä¸æ”¯æŒRPAæœç´¢æ¨¡å¼")

async def run_unified_crawler(keywords: str = None, video_url: str = None, note_url: str = None, creator_url: str = None,
                             crawler_mode: str = "search",
                             shared_context=None, shared_page=None,
                             max_count: int = 20, max_comments_per_video: int = 50, max_comments_per_note: int = 50,
                             enable_comments: bool = True, enable_sub_comments: bool = True,
                             save_format: str = "csv", output_dir: str = None,
                             progress_callback=None, stop_flag_callback=None, platform: str = "dy"):
    """
    è¿è¡Œç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å™¨ - æ”¯æŒä¸‰ç§æ¨¡å¼,æ”¯æŒæŠ–éŸ³å’Œå°çº¢ä¹¦

    Args:
        keywords: æœç´¢å…³é”®è¯ (searchæ¨¡å¼)
        video_url: è§†é¢‘é“¾æ¥ (detailæ¨¡å¼ - æŠ–éŸ³)
        note_url: ç¬”è®°é“¾æ¥ (detailæ¨¡å¼ - å°çº¢ä¹¦)
        creator_url: åˆ›ä½œè€…é“¾æ¥ (creatoræ¨¡å¼)
        crawler_mode: é‡‡é›†æ¨¡å¼ (search/detail/creator)
        shared_context: å…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡
        shared_page: å…±äº«é¡µé¢
        max_count: æœ€å¤§é‡‡é›†æ•°é‡ï¼ˆè§†é¢‘/ç¬”è®°æ•°é‡ï¼‰
        max_comments_per_video: æ¯ä¸ªè§†é¢‘æœ€å¤§è¯„è®ºæ•°é‡ï¼ˆæŠ–éŸ³ï¼‰
        max_comments_per_note: æ¯ä¸ªç¬”è®°æœ€å¤§è¯„è®ºæ•°é‡ï¼ˆå°çº¢ä¹¦ï¼‰
        enable_comments: æ˜¯å¦é‡‡é›†ä¸€çº§è¯„è®º
        enable_sub_comments: æ˜¯å¦é‡‡é›†äºŒçº§è¯„è®º
        save_format: ä¿å­˜æ ¼å¼ (csv/json/sqlite/db)
        output_dir: è¾“å‡ºç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, message)
        stop_flag_callback: åœæ­¢æ ‡å¿—æ£€æŸ¥å‡½æ•° callback() -> bool
        platform: å¹³å° (dy/xhs)

    Returns:
        dict: ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„å­—å…¸ {"contents": "path/to/contents.csv", "comments": "path/to/comments.csv"}
    """
    crawler = UnifiedBrowserCrawler(shared_context, shared_page, progress_callback, stop_flag_callback)

    # ğŸ”¥ æ ¹æ®å¹³å°é€‰æ‹©è¯„è®ºæ•°é‡å‚æ•°
    max_comments = max_comments_per_note if platform == "xhs" else max_comments_per_video
    content_url = note_url if platform == "xhs" else video_url

    if crawler_mode == "search":
        return await crawler.start_search_crawling(
            keywords=keywords,
            max_count=max_count,
            max_comments_per_video=max_comments,
            enable_comments=enable_comments,
            enable_sub_comments=enable_sub_comments,
            save_format=save_format,
            output_dir=output_dir,
            platform=platform  # ğŸ”¥ ä¼ é€’å¹³å°å‚æ•°
        )
    elif crawler_mode == "detail":
        return await crawler.start_detail_crawling(
            video_url=content_url,
            max_comments_per_video=max_comments,
            enable_comments=enable_comments,
            enable_sub_comments=enable_sub_comments,
            save_format=save_format,
            output_dir=output_dir
        )
    elif crawler_mode == "creator":
        return await crawler.start_creator_crawling(
            creator_url=creator_url,
            max_count=max_count,
            max_comments_per_video=max_comments,
            enable_comments=enable_comments,
            enable_sub_comments=enable_sub_comments,
            save_format=save_format,
            output_dir=output_dir
        )
    else:
        raise ValueError(f"æœªçŸ¥çš„é‡‡é›†æ¨¡å¼: {crawler_mode}")

def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ğŸ”¥ ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å™¨")
    parser.add_argument("--keywords", "-k", required=True, help="æœç´¢å…³é”®è¯")
    parser.add_argument("--max-count", "-c", type=int, default=20, help="æœ€å¤§é‡‡é›†æ•°é‡")
    
    args = parser.parse_args()
    
    print("âš ï¸ æ³¨æ„ï¼šæ­¤è„šæœ¬éœ€è¦ä¸GUIåº”ç”¨é…åˆä½¿ç”¨")
    print("è¯·é€šè¿‡GUIåº”ç”¨å¯åŠ¨ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†")

if __name__ == "__main__":
    main()
