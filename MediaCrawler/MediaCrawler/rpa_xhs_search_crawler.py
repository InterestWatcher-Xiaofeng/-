"""
å°çº¢ä¹¦RPAæ¨¡å¼æœç´¢çˆ¬è™« - æ··åˆæ–¹æ¡ˆ
åŠŸèƒ½: ä½¿ç”¨RPAæ¨¡å¼æœç´¢å…³é”®è¯,è·å–ç¬”è®°é“¾æ¥,ç„¶åç”¨Detailæ¨¡å¼æŠ“å–è¯„è®º
"""

import asyncio
import re
import json
from pathlib import Path
from typing import List, Dict
from playwright.async_api import async_playwright, Page, Browser
from datetime import datetime

from config import base_config


class RPAXhsSearchCrawler:
    """å°çº¢ä¹¦RPAæ¨¡å¼æœç´¢çˆ¬è™«"""
    
    def __init__(self, keyword: str, max_notes: int = 20):
        self.keyword = keyword
        self.max_notes = max_notes
        self.note_links = []
        self.browser = None
        self.context = None
        self.page = None
        
    async def start(self):
        """å¯åŠ¨çˆ¬è™«"""
        print("=" * 60)
        print("ğŸš€ å°çº¢ä¹¦RPAæ¨¡å¼æœç´¢çˆ¬è™«å¯åŠ¨")
        print("=" * 60)
        print(f"ğŸ” å…³é”®è¯: {self.keyword}")
        print(f"ğŸ“ ç›®æ ‡ç¬”è®°æ•°: {self.max_notes}")
        print("=" * 60)
        
        async with async_playwright() as playwright:
            # å¯åŠ¨æµè§ˆå™¨
            await self._launch_browser(playwright)
            
            # è®¿é—®å°çº¢ä¹¦æœç´¢é¡µ
            await self._goto_search_page()
            
            # ç­‰å¾…ç”¨æˆ·ç™»å½•
            await self._wait_for_login()
            
            # æ‰§è¡Œæœç´¢
            await self._search_keyword()
            
            # æ»šåŠ¨åŠ è½½ç¬”è®°
            await self._scroll_and_collect_links()
            
            # ä¿å­˜é“¾æ¥
            self._save_links()
            
            # å…³é—­æµè§ˆå™¨
            await self._close_browser()
            
        return self.note_links
    
    async def _launch_browser(self, playwright):
        """å¯åŠ¨æµè§ˆå™¨"""
        print("\nğŸ“± æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        
        self.browser = await playwright.chromium.launch(
            headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨
            channel="chrome"  # ä½¿ç”¨Chrome
        )
        
        # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
        self.context = await self.browser.new_context(
            viewport={"width": 1920, "height": 1080},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        
        # åˆ›å»ºé¡µé¢
        self.page = await self.context.new_page()
        
        print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
    
    async def _goto_search_page(self):
        """è®¿é—®å°çº¢ä¹¦æœç´¢é¡µ"""
        print("\nğŸŒ æ­£åœ¨è®¿é—®å°çº¢ä¹¦æœç´¢é¡µ...")

        url = "https://www.xiaohongshu.com/"
        try:
            await self.page.goto(url, timeout=60000)
            await asyncio.sleep(3)
        except Exception as e:
            print(f"âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶,ç»§ç»­æ‰§è¡Œ: {e}")

        print("âœ… é¡µé¢åŠ è½½å®Œæˆ")
    
    async def _wait_for_login(self):
        """ç­‰å¾…ç”¨æˆ·ç™»å½•"""
        print("\nğŸ” è¯·åœ¨æµè§ˆå™¨ä¸­ç™»å½•å°çº¢ä¹¦...")
        print("   æç¤º: å¦‚æœå·²ç»ç™»å½•,è¯·æŒ‰Enterç»§ç»­")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç™»å½•
        try:
            # ç­‰å¾…ç”¨æˆ·å¤´åƒå…ƒç´ å‡ºç°(è¡¨ç¤ºå·²ç™»å½•)
            await self.page.wait_for_selector(
                'div[class*="avatar"]',
                timeout=60000  # ç­‰å¾…60ç§’
            )
            print("âœ… æ£€æµ‹åˆ°ç™»å½•çŠ¶æ€")
        except:
            print("âš ï¸ æœªæ£€æµ‹åˆ°ç™»å½•,è¯·æ‰‹åŠ¨ç™»å½•åæŒ‰Enterç»§ç»­")
            input("æŒ‰Enterç»§ç»­...")
    
    async def _search_keyword(self):
        """æ‰§è¡Œæœç´¢"""
        print(f"\nğŸ” æ­£åœ¨æœç´¢å…³é”®è¯: {self.keyword}")

        # æ–¹æ³•1: ç›´æ¥è®¿é—®æœç´¢ç»“æœé¡µ
        search_url = f"https://www.xiaohongshu.com/search_result?keyword={self.keyword}"
        try:
            await self.page.goto(search_url, timeout=60000)
            await asyncio.sleep(5)  # ç­‰å¾…æœç´¢ç»“æœåŠ è½½
        except Exception as e:
            print(f"âš ï¸ æœç´¢é¡µé¢åŠ è½½è¶…æ—¶,ç»§ç»­æ‰§è¡Œ: {e}")

        # ğŸ”¥ æ–°å¢: ç‚¹å‡»"æœ€å¤šè¯„è®º"ç­›é€‰
        await self._click_most_comments_filter()

        print("âœ… æœç´¢å®Œæˆ")

    async def _click_most_comments_filter(self):
        """ç‚¹å‡»æœ€å¤šè¯„è®ºç­›é€‰æŒ‰é’®"""
        print("\nğŸ¯ æ­£åœ¨è®¾ç½®ç­›é€‰æ¡ä»¶: æœ€å¤šè¯„è®º...")

        try:
            # ğŸ”¥ ç¬¬1æ­¥: ç‚¹å‡»"ç­›é€‰"æŒ‰é’®
            print("   ğŸ“ æ­¥éª¤1: æŸ¥æ‰¾å¹¶ç‚¹å‡»'ç­›é€‰'æŒ‰é’®...")
            
            filter_selectors = [
                # æ–¹æ³•1: XPath(æœ€ç¨³å®š)
                "//span[contains(text(),'ç­›é€‰')]",
                # æ–¹æ³•2: ç”¨æˆ·æä¾›çš„XPath
                "(//span[contains(text(),'ç­›é€‰')])[1]",
                # æ–¹æ³•3: ç”¨æˆ·æä¾›çš„CSSé€‰æ‹©å™¨
                "body > div:nth-child(2) > div:nth-child(1) > div:nth-child(2) > div:nth-child(2) > div:nth-child(1) > div:nth-child(2) > div:nth-child(1) > div:nth-child(2) > span:nth-child(1)",
                # æ–¹æ³•4: é€šç”¨æ–‡æœ¬åŒ¹é…
                "text=ç­›é€‰",
                # æ–¹æ³•5: åŒ…å«æ–‡æœ¬çš„span
                "span:has-text('ç­›é€‰')",
                # æ–¹æ³•6: ç®€åŒ–çš„CSS
                "div.filter > span"
            ]

            filter_clicked = False
            for idx, selector in enumerate(filter_selectors, 1):
                try:
                    print(f"      å°è¯•æ–¹æ³•{idx}: {selector[:50]}...")
                    filter_element = await self.page.wait_for_selector(selector, timeout=3000)
                    # ğŸ”¥ å…³é”®ä¿®æ”¹: ä½¿ç”¨hover()è€Œä¸æ˜¯click()
                    await filter_element.hover()
                    await asyncio.sleep(1.5)  # ç­‰å¾…ä¸‹æ‹‰èœå•å‡ºç°(å¢åŠ ç­‰å¾…æ—¶é—´)
                    print(f"      âœ… æ–¹æ³•{idx}æˆåŠŸ! å·²æ‚¬åœåœ¨'ç­›é€‰'æŒ‰é’®ä¸Š")
                    filter_clicked = True
                    break
                except Exception as e:
                    print(f"      âŒ æ–¹æ³•{idx}å¤±è´¥: {str(e)[:50]}")
                    continue

            if not filter_clicked:
                print("   âš ï¸ æœªæ‰¾åˆ°'ç­›é€‰'æŒ‰é’®,è·³è¿‡ç­›é€‰,ä½¿ç”¨é»˜è®¤æ’åº")
                return

            # ğŸ”¥ ç¬¬2æ­¥: ç‚¹å‡»"æœ€å¤šè¯„è®º"é€‰é¡¹
            print("   ğŸ“ æ­¥éª¤2: æŸ¥æ‰¾å¹¶ç‚¹å‡»'æœ€å¤šè¯„è®º'é€‰é¡¹...")
            
            most_comments_selectors = [
                # æ–¹æ³•1: æµ‹è¯•ä¸­æˆåŠŸç‡æœ€é«˜çš„é€‰æ‹©å™¨
                "div[class*='filter'] span:has-text('æœ€å¤šè¯„è®º')",
                # æ–¹æ³•2: XPath
                "//span[contains(text(),'æœ€å¤šè¯„è®º')]",
                # æ–¹æ³•3: ç”¨æˆ·æä¾›çš„XPath
                "(//span[contains(text(),'æœ€å¤šè¯„è®º')])[1]",
                # æ–¹æ³•4: é€šç”¨æ–‡æœ¬åŒ¹é…
                "text=æœ€å¤šè¯„è®º",
                # æ–¹æ³•5: åŒ…å«æ–‡æœ¬çš„span
                "span:has-text('æœ€å¤šè¯„è®º')",
                # æ–¹æ³•6: å¤‡ç”¨CSS
                "div[class*='sort'] span:has-text('æœ€å¤šè¯„è®º')"
            ]

            comments_clicked = False
            for idx, selector in enumerate(most_comments_selectors, 1):
                try:
                    print(f"      å°è¯•æ–¹æ³•{idx}: {selector[:50]}...")
                    await self.page.wait_for_selector(selector, timeout=3000)
                    await self.page.click(selector)
                    await asyncio.sleep(2)  # ç­‰å¾…é¡µé¢é‡æ–°åŠ è½½
                    print(f"      âœ… æ–¹æ³•{idx}æˆåŠŸ! å·²é€‰æ‹©'æœ€å¤šè¯„è®º'æ’åº")
                    comments_clicked = True
                    break
                except Exception as e:
                    print(f"      âŒ æ–¹æ³•{idx}å¤±è´¥: {str(e)[:50]}")
                    continue

            if not comments_clicked:
                print("   âš ï¸ æœªæ‰¾åˆ°'æœ€å¤šè¯„è®º'é€‰é¡¹,ä½¿ç”¨é»˜è®¤æ’åº")
            else:
                print("   ğŸ‰ ç­›é€‰è®¾ç½®æˆåŠŸ!")

                # ğŸ”¥ å…³é”®ä¿®å¤: ç‚¹å‡»ç©ºç™½å¤„å…³é—­ç­›é€‰é¢æ¿
                print("   ğŸ“ æ­¥éª¤3: å…³é—­ç­›é€‰é¢æ¿...")
                try:
                    # æ–¹æ³•1: æŒ‰ESCé”®å…³é—­
                    await self.page.keyboard.press('Escape')
                    await asyncio.sleep(0.5)
                    print("      âœ… å·²æŒ‰ESCé”®å…³é—­ç­›é€‰é¢æ¿")
                except Exception as e:
                    print(f"      âš ï¸ æŒ‰ESCé”®å¤±è´¥: {str(e)[:50]}")
                    try:
                        # æ–¹æ³•2: ç‚¹å‡»é¡µé¢ç©ºç™½å¤„
                        await self.page.click('body', position={'x': 100, 'y': 100})
                        await asyncio.sleep(0.5)
                        print("      âœ… å·²ç‚¹å‡»ç©ºç™½å¤„å…³é—­ç­›é€‰é¢æ¿")
                    except Exception as e2:
                        print(f"      âš ï¸ ç‚¹å‡»ç©ºç™½å¤„å¤±è´¥: {str(e2)[:50]}")

        except Exception as e:
            print(f"   âš ï¸ ç­›é€‰è®¾ç½®å¤±è´¥: {e}")
            print("   â„¹ï¸ å°†ä½¿ç”¨é»˜è®¤æ’åºç»§ç»­")
    
    async def _scroll_and_collect_links(self):
        """
        ğŸ”¥ å®Œå…¨æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º:
        1. æ»šåŠ¨æµè§ˆæœç´¢ç»“æœ,ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç¬”è®°å¡ç‰‡å¯è§
        2. ä¸æ”¶é›†note_id,è€Œæ˜¯å‡†å¤‡å¥½ç¬”è®°å¡ç‰‡ä¾›åç»­ç‚¹å‡»
        """
        print(f"\nğŸ“œ å¼€å§‹æ¨¡æ‹ŸçœŸå®ç”¨æˆ·æµè§ˆ...")
        print(f"   ç›®æ ‡: {self.max_notes} ä¸ªç¬”è®°")

        # ğŸ”¥ æ»šåŠ¨é¡µé¢,ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç¬”è®°å¡ç‰‡åŠ è½½å‡ºæ¥
        scroll_count = 0
        max_scrolls = 10  # å‡å°‘æ»šåŠ¨æ¬¡æ•°,åªéœ€è¦ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¡ç‰‡å³å¯

        print(f"\nğŸ“œ æ»šåŠ¨é¡µé¢,åŠ è½½ç¬”è®°å¡ç‰‡...")
        while scroll_count < max_scrolls:
            # è·å–å½“å‰é¡µé¢çš„ç¬”è®°å¡ç‰‡æ•°é‡
            note_cards = await self.page.query_selector_all('a[href*="/explore/"]')
            current_count = len(note_cards)

            print(f"   ğŸ“œ æ»šåŠ¨æ¬¡æ•°: {scroll_count + 1}, å½“å‰å¯è§: {current_count} ä¸ªç¬”è®°")

            # å¦‚æœå·²ç»æœ‰è¶³å¤Ÿçš„ç¬”è®°å¡ç‰‡,åœæ­¢æ»šåŠ¨
            if current_count >= self.max_notes * 2:  # å¤šåŠ è½½ä¸€äº›,ä»¥é˜²æœ‰äº›å¡ç‰‡æ— æ•ˆ
                print(f"   âœ… å·²åŠ è½½è¶³å¤Ÿçš„ç¬”è®°å¡ç‰‡,åœæ­¢æ»šåŠ¨")
                break

            # æ»šåŠ¨é¡µé¢
            await self.page.evaluate("window.scrollBy(0, 800)")
            await asyncio.sleep(1.5)
            scroll_count += 1

        print(f"\nâœ… é¡µé¢å‡†å¤‡å®Œæˆ! å‡†å¤‡ä»å·¦åˆ°å³ä¾æ¬¡ç‚¹å‡»ç¬”è®°...")

        # ğŸ”¥ ä¸å†æ”¶é›†note_id,è€Œæ˜¯æ ‡è®°å‡†å¤‡å¥½äº†
        self.note_links = []  # æ¸…ç©º,ä¸ä½¿ç”¨é“¾æ¥æ¨¡å¼
        self.ready_to_click = True  # æ ‡è®°å‡†å¤‡å¥½ç‚¹å‡»
    async def click_and_scrape_notes(self, xhs_client) -> List[Dict]:
        """
        ğŸ”¥ å®Œå…¨æ¨¡æ‹ŸçœŸå®ç”¨æˆ·: ä»å·¦åˆ°å³ä¾æ¬¡ç‚¹å‡»ç¬”è®°å¡ç‰‡ â†’ æµè§ˆè¯¦æƒ… â†’ æŠ“å–è¯„è®º â†’ è¿”å›æœç´¢é¡µ

        Args:
            xhs_client: å°çº¢ä¹¦å®¢æˆ·ç«¯,ç”¨äºAPIè°ƒç”¨

        Returns:
            ç¬”è®°æ•°æ®åˆ—è¡¨
        """
        import random

        if not hasattr(self, 'ready_to_click') or not self.ready_to_click:
            print("   âš ï¸ é¡µé¢æœªå‡†å¤‡å¥½")
            return []

        all_notes_data = []

        print(f"\nğŸ”¥ å¼€å§‹æ¨¡æ‹ŸçœŸå®ç”¨æˆ·: ä»å·¦åˆ°å³ä¾æ¬¡ç‚¹å‡»ç¬”è®°...")
        print(f"ğŸ“Š ç›®æ ‡: {self.max_notes} ä¸ªç¬”è®°")
        print(f"â° æ¯ä¸ªç¬”è®°ä¹‹é—´å»¶è¿Ÿ 30-45 ç§’,æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º")

        # ä¿å­˜æœç´¢é¡µURL
        search_url = self.page.url

        # ğŸ”¥ æ ¸å¿ƒæ”¹è¿›: é€ä¸ªç‚¹å‡»å¡ç‰‡,è€Œä¸æ˜¯å¯¼èˆªåˆ°URL
        clicked_count = 0

        while clicked_count < self.max_notes:
            try:
                print(f"\n{'='*60}")
                print(f"ğŸ“ [{clicked_count + 1}/{self.max_notes}] å‡†å¤‡ç‚¹å‡»ç¬¬ {clicked_count + 1} ä¸ªç¬”è®°")
                print(f"{'='*60}")

                # ğŸ”¥ æ­¥éª¤1: è·å–å½“å‰é¡µé¢çš„æ‰€æœ‰ç¬”è®°å¡ç‰‡
                print(f"   ğŸ” æŸ¥æ‰¾ç¬”è®°å¡ç‰‡...")
                note_cards = await self.page.query_selector_all('a[href*="/explore/"]')

                if not note_cards or len(note_cards) <= clicked_count:
                    print(f"   âš ï¸ æ²¡æœ‰æ›´å¤šç¬”è®°å¡ç‰‡äº†")
                    break

                # ğŸ”¥ æ­¥éª¤2: è·å–ç¬¬Nä¸ªå¡ç‰‡(ä»å·¦åˆ°å³,ä»ä¸Šåˆ°ä¸‹çš„é¡ºåº)
                target_card = note_cards[clicked_count]

                # è·å–note_idç”¨äºæ—¥å¿—
                href = await target_card.get_attribute('href')
                note_id = href.split('/explore/')[1].split('?')[0] if href and '/explore/' in href else 'unknown'

                print(f"   ğŸ¯ ç›®æ ‡ç¬”è®°: {note_id}")

                # ğŸ”¥ æ­¥éª¤3: æ»šåŠ¨åˆ°å¡ç‰‡å¯è§åŒºåŸŸ
                print(f"   ğŸ“œ æ»šåŠ¨åˆ°ç¬”è®°å¡ç‰‡...")
                await target_card.scroll_into_view_if_needed(timeout=5000)
                await asyncio.sleep(random.uniform(0.5, 1))

                # ğŸ”¥ æ­¥éª¤4: ç‚¹å‡»å¡ç‰‡(æ¨¡æ‹ŸçœŸå®ç”¨æˆ·ç‚¹å‡»)
                print(f"   ğŸ‘† ç‚¹å‡»ç¬”è®°å¡ç‰‡...")
                try:
                    # å°è¯•æ™®é€šç‚¹å‡»
                    await target_card.click(timeout=5000)
                except Exception as e:
                    print(f"   âš ï¸ æ™®é€šç‚¹å‡»å¤±è´¥,å°è¯•å¼ºåˆ¶ç‚¹å‡»: {str(e)[:50]}")
                    # å¦‚æœæ™®é€šç‚¹å‡»å¤±è´¥,ä½¿ç”¨JavaScriptç‚¹å‡»
                    await target_card.evaluate('element => element.click()')

                # ğŸ”¥ æ­¥éª¤5: ç­‰å¾…é¡µé¢è·³è½¬åˆ°è¯¦æƒ…é¡µ
                print(f"   â³ ç­‰å¾…é¡µé¢è·³è½¬...")
                await asyncio.sleep(random.uniform(2, 3))

                # ğŸ”¥ æ­¥éª¤6: ä»åœ°å€æ è·å–å®Œæ•´URL(åŒ…å«xsecå‚æ•°)
                current_url = self.page.url

                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·³è½¬åˆ°è¯¦æƒ…é¡µ
                if '/explore/' not in current_url or '/404' in current_url:
                    print(f"   âŒ é¡µé¢è·³è½¬å¤±è´¥æˆ–è¢«é‡å®šå‘åˆ°404")
                    print(f"   ğŸ”— å½“å‰URL: {current_url[:80]}...")
                    # è¿”å›æœç´¢é¡µ
                    await self.page.go_back(wait_until='domcontentloaded', timeout=10000)
                    await asyncio.sleep(2)
                    clicked_count += 1
                    continue

                # è§£æURLè·å–xsecå‚æ•°
                from urllib.parse import urlparse, parse_qs
                parsed = urlparse(current_url)
                query_params = parse_qs(parsed.query)
                xsec_token = query_params.get('xsec_token', [''])[0]
                xsec_source = query_params.get('xsec_source', [''])[0]

                print(f"   âœ… æˆåŠŸè¿›å…¥ç¬”è®°è¯¦æƒ…é¡µ")
                print(f"   ğŸ”— å®Œæ•´URL: {current_url[:80]}...")
                print(f"   ğŸ”‘ xsec_token: {xsec_token[:20]}..." if xsec_token else "   âš ï¸ ç¼ºå°‘xsec_token")

                # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·æµè§ˆç¬”è®°
                browse_delay = random.uniform(3, 5)
                print(f"   ğŸ‘€ æµè§ˆç¬”è®°å†…å®¹ ({browse_delay:.1f}ç§’)...")
                await asyncio.sleep(browse_delay)

                # ğŸ”¥ æ­¥éª¤7: æŠ“å–ç¬”è®°è¯¦æƒ…
                print(f"   ğŸ“„ æ­£åœ¨è·å–ç¬”è®°è¯¦æƒ…...")
                note_detail = await xhs_client.get_note_by_id_from_html(current_url)

                if not note_detail:
                    print(f"   âŒ ç¬”è®°è¯¦æƒ…è·å–å¤±è´¥")
                    # è¿”å›æœç´¢é¡µ
                    await self.page.go_back(wait_until='domcontentloaded', timeout=10000)
                    await asyncio.sleep(2)
                    clicked_count += 1
                    continue

                print(f"   âœ… ç¬”è®°è¯¦æƒ…è·å–æˆåŠŸ")
                print(f"   ğŸ“ æ ‡é¢˜: {note_detail.get('title', 'N/A')[:30]}...")
                print(f"   ğŸ‘¤ ä½œè€…: {note_detail.get('nickname', 'N/A')}")

                # ğŸ”¥ æ­¥éª¤8: æ»šåŠ¨é¡µé¢,æ¨¡æ‹ŸæŸ¥çœ‹è¯„è®º
                print(f"   ğŸ“œ æ»šåŠ¨æŸ¥çœ‹è¯„è®ºåŒº...")
                await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(random.uniform(2, 3))

                # ğŸ”¥ æ­¥éª¤9: æŠ“å–è¯„è®º
                print(f"   ğŸ’¬ æ­£åœ¨è·å–è¯„è®º...")
                comments = await xhs_client.get_note_comments(
                    note_id=note_id,
                    xsec_token=xsec_token,
                    xsec_source=xsec_source
                )

                if comments:
                    print(f"   âœ… æˆåŠŸè·å– {len(comments)} æ¡è¯„è®º")
                else:
                    print(f"   âš ï¸ æœªè·å–åˆ°è¯„è®º")

                # ä¿å­˜æ•°æ®
                note_data = {
                    'note_id': note_id,
                    'note_url': current_url,
                    'note_detail': note_detail,
                    'comments': comments or []
                }
                all_notes_data.append(note_data)

                # ğŸ”¥ æ­¥éª¤10: è¿”å›æœç´¢é¡µ(ä½¿ç”¨æµè§ˆå™¨åé€€æŒ‰é’®)
                if clicked_count < self.max_notes - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ª
                    # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¿”å›
                    return_delay = random.uniform(30, 45)
                    print(f"   â° ç­‰å¾… {return_delay:.1f} ç§’åè¿”å›æœç´¢é¡µ...")
                    await asyncio.sleep(return_delay)

                    print(f"   â†©ï¸ ç‚¹å‡»æµè§ˆå™¨åé€€æŒ‰é’®...")
                    await self.page.go_back(wait_until='domcontentloaded', timeout=10000)
                    await asyncio.sleep(random.uniform(2, 3))

                    print(f"   âœ… å·²è¿”å›æœç´¢é¡µ,å‡†å¤‡ç‚¹å‡»ä¸‹ä¸€ä¸ªç¬”è®°")

                clicked_count += 1

            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {str(e)[:100]}")
                # å°è¯•è¿”å›æœç´¢é¡µ
                try:
                    print(f"   â†©ï¸ å°è¯•è¿”å›æœç´¢é¡µ...")
                    await self.page.go_back(wait_until='domcontentloaded', timeout=10000)
                    await asyncio.sleep(2)
                except:
                    # å¦‚æœåé€€å¤±è´¥,ç›´æ¥å¯¼èˆªåˆ°æœç´¢é¡µ
                    try:
                        await self.page.goto(search_url, wait_until='domcontentloaded', timeout=10000)
                        await asyncio.sleep(2)
                    except:
                        pass
                clicked_count += 1
                continue

        print(f"\n{'='*60}")
        print(f"ğŸ‰ æ‰€æœ‰ç¬”è®°å¤„ç†å®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {len(all_notes_data)}/{self.max_notes} ä¸ªç¬”è®°")
        print(f"{'='*60}")

        return all_notes_data

    async def _extract_note_links_by_clicking(self, note_ids: List[str]) -> List[str]:
        """
        é€šè¿‡ç‚¹å‡»ç¬”è®°å¡ç‰‡è·å–å®Œæ•´URL(åŒ…å«xsec_tokenå’Œxsec_source)

        Args:
            note_ids: è¦è·å–å®Œæ•´URLçš„note_idåˆ—è¡¨

        Returns:
            åŒ…å«å®Œæ•´å‚æ•°çš„URLåˆ—è¡¨
        """
        import random

        links = []
        processed_note_ids = set()

        print(f"   éœ€è¦è·å– {len(note_ids)} ä¸ªç¬”è®°çš„å®Œæ•´é“¾æ¥")

        # ğŸ”¥ é€ä¸ªç‚¹å‡»è·å–å®Œæ•´URL
        for index, target_note_id in enumerate(note_ids, 1):
            if target_note_id in processed_note_ids:
                continue

            try:
                # ğŸ”¥ æ¯æ¬¡éƒ½é‡æ–°è·å–å…ƒç´ 
                note_cards = await self.page.query_selector_all('a[href*="/explore/"]')

                # æ‰¾åˆ°ç›®æ ‡note_idçš„å¡ç‰‡
                target_card = None
                for card in note_cards:
                    try:
                        href = await card.get_attribute('href')
                        if href and target_note_id in href:
                            target_card = card
                            break
                    except:
                        continue

                if not target_card:
                    print(f"   âš ï¸ [{index}/{len(note_ids)}] æœªæ‰¾åˆ°ç¬”è®° {target_note_id} çš„å¡ç‰‡")
                    continue

                # ğŸ”¥ æ»šåŠ¨åˆ°å…ƒç´ å¯è§
                try:
                    await target_card.scroll_into_view_if_needed(timeout=5000)
                    await asyncio.sleep(0.5)
                except:
                    pass

                # ğŸ”¥ å°è¯•ç‚¹å‡»(ä½¿ç”¨force=Trueå¼ºåˆ¶ç‚¹å‡»)
                try:
                    await target_card.click(timeout=10000, force=True)
                except Exception as click_error:
                    # å¦‚æœæ™®é€šç‚¹å‡»å¤±è´¥,å°è¯•JavaScriptç‚¹å‡»
                    try:
                        await target_card.evaluate('element => element.click()')
                    except:
                        print(f"   âŒ [{index}/{len(note_ids)}] ç‚¹å‡»å¤±è´¥: {str(click_error)[:40]}")
                        continue

                # ç­‰å¾…é¡µé¢è·³è½¬
                delay = random.uniform(2, 3)
                await asyncio.sleep(delay)

                # è·å–å®Œæ•´URL
                full_url = self.page.url

                # éªŒè¯URL
                if '/explore/' in full_url and 'xsec_token=' in full_url and 'xsec_source=' in full_url:
                    links.append(full_url)
                    processed_note_ids.add(target_note_id)
                    print(f"   âœ… [{index}/{len(note_ids)}] å·²è·å–: {target_note_id}")
                else:
                    print(f"   âš ï¸ [{index}/{len(note_ids)}] URLç¼ºå°‘å‚æ•°: {full_url[:60]}")

                # è¿”å›æœç´¢é¡µ
                await self.page.go_back()

                # ç­‰å¾…é¡µé¢åŠ è½½
                delay = random.uniform(1.5, 2.5)
                await asyncio.sleep(delay)

            except Exception as e:
                print(f"   âŒ [{index}/{len(note_ids)}] è·å–å¤±è´¥: {str(e)[:40]}")
                # å°è¯•è¿”å›æœç´¢é¡µ
                try:
                    current_url = self.page.url
                    if '/explore/' in current_url:
                        await self.page.go_back()
                        await asyncio.sleep(1)
                except:
                    pass
                continue

        return links
    
    def _save_links(self):
        """ä¿å­˜é“¾æ¥åˆ°é…ç½®æ–‡ä»¶"""
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜é“¾æ¥åˆ°é…ç½®æ–‡ä»¶...")
        
        # ä¿å­˜åˆ° config/xhs_config.py
        config_file = Path(__file__).parent / "config" / "xhs_config.py"
        
        try:
            # è¯»å–ç°æœ‰é…ç½®
            with open(config_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ„å»ºæ–°çš„é“¾æ¥åˆ—è¡¨
            links_str = '[\n'
            for link in self.note_links:
                links_str += f'    "{link}",\n'
            links_str += ']'
            
            # æ›¿æ¢ XHS_SPECIFIED_NOTE_URL_LIST
            import re
            pattern = r'XHS_SPECIFIED_NOTE_URL_LIST\s*=\s*\[.*?\]'
            replacement = f'XHS_SPECIFIED_NOTE_URL_LIST = {links_str}'
            
            new_content = re.sub(pattern, replacement, content, flags=re.DOTALL)
            
            # å†™å›æ–‡ä»¶
            with open(config_file, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            print(f"âœ… å·²ä¿å­˜ {len(self.note_links)} ä¸ªé“¾æ¥åˆ° {config_file}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜é“¾æ¥å¤±è´¥: {e}")
            print("   é“¾æ¥åˆ—è¡¨:")
            for link in self.note_links:
                print(f"   - {link}")
    
    async def _close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        print("\nğŸ”’ æ­£åœ¨å…³é—­æµè§ˆå™¨...")
        
        if self.browser:
            await self.browser.close()
        
        print("âœ… æµè§ˆå™¨å·²å…³é—­")


async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ¯" * 30)
    print("å°çº¢ä¹¦RPAæ¨¡å¼æœç´¢çˆ¬è™« - æ··åˆæ–¹æ¡ˆ")
    print("ğŸ¯" * 30)
    
    # ä»é…ç½®è¯»å–å‚æ•°
    keyword = base_config.KEYWORDS.split(',')[0].strip()  # å–ç¬¬ä¸€ä¸ªå…³é”®è¯
    max_notes = base_config.CRAWLER_MAX_NOTES_COUNT
    
    print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   å…³é”®è¯: {keyword}")
    print(f"   ç¬”è®°æ•°é‡: {max_notes}")
    
    # åˆ›å»ºçˆ¬è™«
    crawler = RPAXhsSearchCrawler(keyword=keyword, max_notes=max_notes)
    
    # æ‰§è¡Œæœç´¢
    note_links = await crawler.start()
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ‰ RPAæœç´¢å®Œæˆ!")
    print("=" * 60)
    print(f"âœ… å…±æ”¶é›† {len(note_links)} ä¸ªç¬”è®°é“¾æ¥")
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("   1. ç¬”è®°é“¾æ¥å·²è‡ªåŠ¨æ›´æ–°åˆ° config/xhs_config.py")
    print("   2. è¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹æŠ“å–è¯„è®º:")
    print("      python main.py --platform xhs --lt qrcode --type detail")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())

