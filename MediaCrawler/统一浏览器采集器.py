#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å™¨
è§£å†³GUIç™»å½•å’Œçˆ¬è™«é‡‡é›†ä½¿ç”¨ä¸åŒæµè§ˆå™¨å®ä¾‹çš„é—®é¢˜
ä½¿ç”¨åŒä¸€ä¸ªæµè§ˆå™¨çª—å£è¿›è¡Œç™»å½•å’Œåç»­æ•°æ®é‡‡é›†
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

import config
from media_platform.douyin.core import DouYinCrawler
from tools.utils import utils

class UnifiedBrowserCrawler:
    """ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å™¨"""
    
    def __init__(self, shared_context=None, shared_page=None, progress_callback=None):
        """
        åˆå§‹åŒ–ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å™¨

        Args:
            shared_context: GUIæä¾›çš„å…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡
            shared_page: GUIæä¾›çš„å…±äº«é¡µé¢
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, message)
        """
        self.shared_context = shared_context
        self.shared_page = shared_page
        self.crawler = None
        self.progress_callback = progress_callback
        
    async def setup_crawler(self, platform: str = "dy"):
        """è®¾ç½®çˆ¬è™«å®ä¾‹"""
        if platform == "dy":
            self.crawler = DouYinCrawler()
            # ğŸ”¥ å…³é”®ï¼šå°†å…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡æ³¨å…¥åˆ°çˆ¬è™«ä¸­
            if hasattr(self.crawler, 'browser_context'):
                self.crawler.browser_context = self.shared_context
            if hasattr(self.crawler, 'context'):
                self.crawler.context = self.shared_context
        else:
            raise ValueError(f"æš‚ä¸æ”¯æŒå¹³å°: {platform}")
    
    async def start_search_crawling(self, keywords: str, max_count: int = 20,
                                    max_comments_per_video: int = 50,
                                    enable_comments: bool = True,
                                    enable_sub_comments: bool = True,
                                    save_format: str = "csv",
                                    output_dir: str = None):
        """
        å¼€å§‹æœç´¢é‡‡é›†

        Args:
            keywords: æœç´¢å…³é”®è¯
            max_count: æœ€å¤§é‡‡é›†æ•°é‡ï¼ˆè§†é¢‘æ•°é‡ï¼‰
            max_comments_per_video: æ¯ä¸ªè§†é¢‘æœ€å¤§è¯„è®ºæ•°é‡
            enable_comments: æ˜¯å¦é‡‡é›†ä¸€çº§è¯„è®º
            enable_sub_comments: æ˜¯å¦é‡‡é›†äºŒçº§è¯„è®º
            save_format: ä¿å­˜æ ¼å¼ (csv/json/sqlite/db)
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç›®å½•ï¼‰
        """
        try:
            # ğŸ”¥ è®¾ç½®å®Œæ•´é…ç½®
            config.KEYWORDS = keywords
            config.CRAWLER_MAX_NOTES_COUNT = max_count
            config.CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = max_comments_per_video
            config.CRAWLER_TYPE = "search"
            config.PLATFORM = "dy"
            config.ENABLE_GET_COMMENTS = enable_comments
            config.ENABLE_GET_SUB_COMMENTS = enable_sub_comments
            config.SAVE_DATA_OPTION = save_format

            # ğŸ”¥ æ¯æ¬¡é‡‡é›†å‰é‡ç½®storeå®ä¾‹å’Œè§†é¢‘ä¿¡æ¯ç¼“å­˜
            # è¿™æ ·å¯ä»¥ç¡®ä¿æ¯æ¬¡é‡‡é›†éƒ½åˆ›å»ºæ–°çš„æ–‡ä»¶ï¼ˆå¸¦æ–°çš„æ—¶é—´æˆ³ï¼‰
            from store.douyin import DouyinStoreFactory
            import store.douyin as douyin_store

            DouyinStoreFactory.reset_store()  # é‡ç½®storeå®ä¾‹
            douyin_store._video_info_cache.clear()  # æ¸…ç©ºè§†é¢‘ä¿¡æ¯ç¼“å­˜

            # ğŸ”¥ è®¾ç½®è¾“å‡ºç›®å½•
            if output_dir:
                DouyinStoreFactory.set_output_dir(output_dir)
                print(f"ğŸ“ è®¾ç½®è¾“å‡ºç›®å½•: {output_dir}")

            print(f"ğŸš€ å¼€å§‹ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†")
            print(f"ğŸ” å…³é”®è¯: {keywords}")
            print(f"ğŸ“Š è§†é¢‘æ•°é‡: {max_count} ä¸ª")
            print(f"ğŸ’¬ æ¯ä¸ªè§†é¢‘è¯„è®ºæ•°: {max_comments_per_video} æ¡")
            print(f"âœ… ä¸€çº§è¯„è®º: {enable_comments}")
            print(f"âœ… äºŒçº§è¯„è®º: {enable_sub_comments}")
            print(f"ğŸ’¾ ä¿å­˜æ ¼å¼: {save_format}")
            print(f"ğŸ”¥ ä½¿ç”¨å…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡")

            # è®¾ç½®çˆ¬è™«
            await self.setup_crawler("dy")

            # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨ç»Ÿä¸€æµè§ˆå™¨è¿›è¡Œé‡‡é›†
            if self.crawler:
                await self.start_unified_douyin_crawling()

            print(f"âœ… é‡‡é›†å®Œæˆï¼")

            # ğŸ”¥ è¿”å›ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„ï¼ˆä¼ é€’å…³é”®è¯ç”¨äºæ–‡ä»¶å‘½åï¼‰
            return self._get_generated_files(save_format, output_dir, keywords)

        except Exception as e:
            print(f"âŒ é‡‡é›†å¤±è´¥: {str(e)}")
            raise

    def _get_generated_files(self, save_format: str, output_dir: str = None, keywords: str = "") -> dict:
        """
        è·å–ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„

        ğŸ”¥ æ–°å‘½åè§„åˆ™ï¼šå¹³å°_å…³é”®è¯.æ ¼å¼
        ç¤ºä¾‹ï¼šæŠ–éŸ³_ç¾é£Ÿæ¢åº—.csv
        """
        import os
        import re
        from tools.utils import utils

        if output_dir:
            base_path = output_dir
        else:
            base_path = f"data/douyin/{save_format}"

        # ğŸ”¥ æ¸…ç†å…³é”®è¯ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        clean_keywords = re.sub(r'[\\/:*?"<>|\s]+', '_', keywords.strip())
        if not clean_keywords:
            clean_keywords = "æœªå‘½å"

        # ğŸ”¥ æ–°å‘½åæ ¼å¼ï¼šå¹³å°_å…³é”®è¯
        platform_name = "æŠ–éŸ³"
        files = {
            "contents": f"{base_path}/{platform_name}_{clean_keywords}_å†…å®¹.{save_format}",
            "comments": f"{base_path}/{platform_name}_{clean_keywords}_è¯„è®º.{save_format}"
        }

        # åªè¿”å›å­˜åœ¨çš„æ–‡ä»¶
        existing_files = {}
        for key, path in files.items():
            if os.path.exists(path):
                existing_files[key] = path
                print(f"ğŸ“„ {key}æ–‡ä»¶: {path}")

        return existing_files

    async def start_unified_douyin_crawling(self):
        """ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€æµè§ˆå™¨è¿›è¡ŒæŠ–éŸ³é‡‡é›†"""
        try:
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ‡è®°è¿™æ˜¯ç»Ÿä¸€æµè§ˆå™¨æ¨¡å¼ï¼Œä¸è¦å…³é—­æµè§ˆå™¨ä¸Šä¸‹æ–‡
            self.crawler._is_unified_browser = True

            # ç›´æ¥è®¾ç½®æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡æµè§ˆå™¨å¯åŠ¨
            self.crawler.browser_context = self.shared_context
            self.crawler.context_page = self.shared_page

            # ğŸ”¥ ä¼ é€’è¿›åº¦å›è°ƒç»™çˆ¬è™«
            if self.progress_callback:
                self.crawler.progress_callback = self.progress_callback

            # ğŸ”¥ æ¯æ¬¡é‡‡é›†éƒ½é‡æ–°åˆ›å»ºæŠ–éŸ³å®¢æˆ·ç«¯ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„cookies
            from media_platform.douyin.client import DouYinClient
            print("ğŸ”„ åˆ›å»ºæ–°çš„æŠ–éŸ³å®¢æˆ·ç«¯...")
            self.crawler.dy_client = await self.crawler.create_douyin_client(None)

            # æ£€æŸ¥ç™»å½•çŠ¶æ€
            print("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
            if not await self.crawler.dy_client.pong(browser_context=self.shared_context):
                print("âš ï¸ ç™»å½•çŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œä½†ç»§ç»­å°è¯•é‡‡é›†...")

            # æ›´æ–°å®¢æˆ·ç«¯cookies
            print("ğŸª æ›´æ–°å®¢æˆ·ç«¯cookies...")
            await self.crawler.dy_client.update_cookies(browser_context=self.shared_context)

            # å¼€å§‹æœç´¢
            from var import crawler_type_var
            crawler_type_var.set(config.CRAWLER_TYPE)

            print("ğŸ” å¼€å§‹æœç´¢é‡‡é›†...")
            await self.crawler.search()

            print("âœ… æœç´¢é‡‡é›†å®Œæˆ")

        except Exception as e:
            print(f"âŒ ç»Ÿä¸€æµè§ˆå™¨æŠ–éŸ³é‡‡é›†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

async def run_unified_crawler(keywords: str, shared_context=None, shared_page=None,
                             max_count: int = 20, max_comments_per_video: int = 50,
                             enable_comments: bool = True, enable_sub_comments: bool = True,
                             save_format: str = "csv", output_dir: str = None,
                             progress_callback=None):
    """
    è¿è¡Œç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å™¨

    Args:
        keywords: æœç´¢å…³é”®è¯
        shared_context: å…±äº«æµè§ˆå™¨ä¸Šä¸‹æ–‡
        shared_page: å…±äº«é¡µé¢
        max_count: æœ€å¤§é‡‡é›†æ•°é‡ï¼ˆè§†é¢‘æ•°é‡ï¼‰
        max_comments_per_video: æ¯ä¸ªè§†é¢‘æœ€å¤§è¯„è®ºæ•°é‡
        enable_comments: æ˜¯å¦é‡‡é›†ä¸€çº§è¯„è®º
        enable_sub_comments: æ˜¯å¦é‡‡é›†äºŒçº§è¯„è®º
        save_format: ä¿å­˜æ ¼å¼ (csv/json/sqlite/db)
        output_dir: è¾“å‡ºç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, message)

    Returns:
        dict: ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„å­—å…¸ {"contents": "path/to/contents.csv", "comments": "path/to/comments.csv"}
    """
    crawler = UnifiedBrowserCrawler(shared_context, shared_page, progress_callback)
    return await crawler.start_search_crawling(
        keywords=keywords,
        max_count=max_count,
        max_comments_per_video=max_comments_per_video,
        enable_comments=enable_comments,
        enable_sub_comments=enable_sub_comments,
        save_format=save_format,
        output_dir=output_dir
    )

def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ğŸ”¥ ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†å™¨")
    parser.add_argument("--keywords", "-k", required=True, help="æœç´¢å…³é”®è¯")
    parser.add_argument("--max-count", "-c", type=int, default=20, help="æœ€å¤§é‡‡é›†æ•°é‡")
    
    args = parser.parse_args()
    
    print("âš ï¸ æ³¨æ„ï¼šæ­¤è„šæœ¬éœ€è¦ä¸GUIåº”ç”¨é…åˆä½¿ç”¨")
    print("è¯·é€šè¿‡GUIåº”ç”¨å¯åŠ¨ç»Ÿä¸€æµè§ˆå™¨é‡‡é›†")

if __name__ == "__main__":
    main()
