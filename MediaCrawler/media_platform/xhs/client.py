# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚

import asyncio
import json
import re
from typing import Any, Callable, Dict, List, Optional, Union
from urllib.parse import urlencode

import httpx
from playwright.async_api import BrowserContext, Page
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_result

import config
from base.base_crawler import AbstractApiClient
from tools import utils
from html import unescape

from .exception import DataFetchError, IPBlockError
from .field import SearchNoteType, SearchSortType
from .help import get_search_id, sign
from .extractor import XiaoHongShuExtractor


class XiaoHongShuClient(AbstractApiClient):

    def __init__(
        self,
        timeout=60,  # è‹¥å¼€å¯çˆ¬å–åª’ä½“é€‰é¡¹ï¼Œxhs çš„é•¿è§†é¢‘éœ€è¦æ›´ä¹…çš„è¶…æ—¶æ—¶é—´
        proxy=None,
        *,
        headers: Dict[str, str],
        playwright_page: Page,
        cookie_dict: Dict[str, str],
    ):
        self.proxy = proxy
        self.timeout = timeout
        self.headers = headers
        self._host = "https://edith.xiaohongshu.com"
        self._domain = "https://www.xiaohongshu.com"
        self.IP_ERROR_STR = "ç½‘ç»œè¿æ¥å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®æˆ–é‡å¯è¯•è¯•"
        self.IP_ERROR_CODE = 300012
        self.NOTE_ABNORMAL_STR = "ç¬”è®°çŠ¶æ€å¼‚å¸¸ï¼Œè¯·ç¨åæŸ¥çœ‹"
        self.NOTE_ABNORMAL_CODE = -510001
        self.playwright_page = playwright_page
        self.cookie_dict = cookie_dict
        self._extractor = XiaoHongShuExtractor()

    async def _pre_headers(self, url: str, data=None) -> Dict:
        """
        è¯·æ±‚å¤´å‚æ•°ç­¾å
        Args:
            url:
            data:

        Returns:

        """
        # ğŸ”¥ ç­‰å¾…window._webmsxywå‡½æ•°åŠ è½½å®Œæˆ
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # æ£€æŸ¥å‡½æ•°æ˜¯å¦å­˜åœ¨
                func_exists = await self.playwright_page.evaluate(
                    "() => typeof window._webmsxyw === 'function'"
                )

                if not func_exists:
                    if attempt < max_retries - 1:
                        utils.logger.warning(f"[XiaoHongShuClient._pre_headers] window._webmsxyw not ready, retry {attempt + 1}/{max_retries}")
                        await asyncio.sleep(1)
                        continue
                    else:
                        raise Exception("window._webmsxyw function not found after retries")

                # è°ƒç”¨åŠ å¯†å‡½æ•°
                encrypt_params = await self.playwright_page.evaluate(
                    "([url, data]) => window._webmsxyw(url,data)", [url, data]
                )
                break

            except Exception as e:
                if attempt < max_retries - 1:
                    utils.logger.warning(f"[XiaoHongShuClient._pre_headers] Attempt {attempt + 1} failed: {e}, retrying...")
                    await asyncio.sleep(1)
                else:
                    raise

        local_storage = await self.playwright_page.evaluate("() => window.localStorage")
        signs = sign(
            a1=self.cookie_dict.get("a1", ""),
            b1=local_storage.get("b1", ""),
            x_s=encrypt_params.get("X-s", ""),
            x_t=str(encrypt_params.get("X-t", "")),
        )

        headers = {
            "X-S": signs["x-s"],
            "X-T": signs["x-t"],
            "x-S-Common": signs["x-s-common"],
            "X-B3-Traceid": signs["x-b3-traceid"],
        }
        self.headers.update(headers)
        return self.headers

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(1))
    async def request(self, method, url, **kwargs) -> Union[str, Any]:
        """
        å°è£…httpxçš„å…¬å…±è¯·æ±‚æ–¹æ³•ï¼Œå¯¹è¯·æ±‚å“åº”åšä¸€äº›å¤„ç†
        Args:
            method: è¯·æ±‚æ–¹æ³•
            url: è¯·æ±‚çš„URL
            **kwargs: å…¶ä»–è¯·æ±‚å‚æ•°ï¼Œä¾‹å¦‚è¯·æ±‚å¤´ã€è¯·æ±‚ä½“ç­‰

        Returns:

        """
        # return response.text
        return_response = kwargs.pop("return_response", False)
        async with httpx.AsyncClient(proxy=self.proxy) as client:
            response = await client.request(method, url, timeout=self.timeout, **kwargs)

        if response.status_code == 471 or response.status_code == 461:
            # someday someone maybe will bypass captcha
            verify_type = response.headers["Verifytype"]
            verify_uuid = response.headers["Verifyuuid"]
            msg = f"å‡ºç°éªŒè¯ç ï¼Œè¯·æ±‚å¤±è´¥ï¼ŒVerifytype: {verify_type}ï¼ŒVerifyuuid: {verify_uuid}, Response: {response}"
            utils.logger.error(msg)
            raise Exception(msg)

        if return_response:
            return response.text
        data: Dict = response.json()
        if data["success"]:
            return data.get("data", data.get("success", {}))
        elif data["code"] == self.IP_ERROR_CODE:
            raise IPBlockError(self.IP_ERROR_STR)
        else:
            raise DataFetchError(data.get("msg", None))

    async def get(self, uri: str, params=None) -> Dict:
        """
        GETè¯·æ±‚ï¼Œå¯¹è¯·æ±‚å¤´ç­¾å
        Args:
            uri: è¯·æ±‚è·¯ç”±
            params: è¯·æ±‚å‚æ•°

        Returns:

        """
        final_uri = uri
        if isinstance(params, dict):
            final_uri = f"{uri}?" f"{urlencode(params)}"
        headers = await self._pre_headers(final_uri)
        return await self.request(
            method="GET", url=f"{self._host}{final_uri}", headers=headers
        )

    async def post(self, uri: str, data: dict, **kwargs) -> Dict:
        """
        POSTè¯·æ±‚ï¼Œå¯¹è¯·æ±‚å¤´ç­¾å
        Args:
            uri: è¯·æ±‚è·¯ç”±
            data: è¯·æ±‚ä½“å‚æ•°

        Returns:

        """
        headers = await self._pre_headers(uri, data)
        json_str = json.dumps(data, separators=(",", ":"), ensure_ascii=False)
        return await self.request(
            method="POST",
            url=f"{self._host}{uri}",
            data=json_str,
            headers=headers,
            **kwargs,
        )

    async def get_note_media(self, url: str) -> Union[bytes, None]:
        async with httpx.AsyncClient(proxy=self.proxy) as client:
            try:
                response = await client.request("GET", url, timeout=self.timeout)
                response.raise_for_status()
                if not response.reason_phrase == "OK":
                    utils.logger.error(
                        f"[XiaoHongShuClient.get_note_media] request {url} err, res:{response.text}"
                    )
                    return None
                else:
                    return response.content
            except (
                httpx.HTTPError
            ) as exc:  # some wrong when call httpx.request method, such as connection error, client error, server error or response status code is not 2xx
                utils.logger.error(
                    f"[XiaoHongShuClient.get_aweme_media] {exc.__class__.__name__} for {exc.request.url} - {exc}"
                )  # ä¿ç•™åŸå§‹å¼‚å¸¸ç±»å‹åç§°ï¼Œä»¥ä¾¿å¼€å‘è€…è°ƒè¯•
                return None

    async def pong(self) -> bool:
        """
        ç”¨äºæ£€æŸ¥ç™»å½•æ€æ˜¯å¦å¤±æ•ˆäº†
        Returns:

        """
        """get a note to check if login state is ok"""
        utils.logger.info("[XiaoHongShuClient.pong] Begin to pong xhs...")
        ping_flag = False
        try:
            note_card: Dict = await self.get_note_by_keyword(keyword="å°çº¢ä¹¦")
            if note_card.get("items"):
                ping_flag = True
        except Exception as e:
            utils.logger.error(
                f"[XiaoHongShuClient.pong] Ping xhs failed: {e}, and try to login again..."
            )
            ping_flag = False
        return ping_flag

    async def update_cookies(self, browser_context: BrowserContext):
        """
        APIå®¢æˆ·ç«¯æä¾›çš„æ›´æ–°cookiesæ–¹æ³•ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ç™»å½•æˆåŠŸåä¼šè°ƒç”¨æ­¤æ–¹æ³•
        Args:
            browser_context: æµè§ˆå™¨ä¸Šä¸‹æ–‡å¯¹è±¡

        Returns:

        """
        cookie_str, cookie_dict = utils.convert_cookies(await browser_context.cookies())
        self.headers["Cookie"] = cookie_str
        self.cookie_dict = cookie_dict

    async def get_note_by_keyword(
        self,
        keyword: str,
        search_id: str = get_search_id(),
        page: int = 1,
        page_size: int = 20,
        sort: SearchSortType = SearchSortType.GENERAL,
        note_type: SearchNoteType = SearchNoteType.ALL,
    ) -> Dict:
        """
        æ ¹æ®å…³é”®è¯æœç´¢ç¬”è®°
        Args:
            keyword: å…³é”®è¯å‚æ•°
            page: åˆ†é¡µç¬¬å‡ é¡µ
            page_size: åˆ†é¡µæ•°æ®é•¿åº¦
            sort: æœç´¢ç»“æœæ’åºæŒ‡å®š
            note_type: æœç´¢çš„ç¬”è®°ç±»å‹

        Returns:

        """
        uri = "/api/sns/web/v1/search/notes"
        data = {
            "keyword": keyword,
            "page": page,
            "page_size": page_size,
            "search_id": search_id,
            "sort": sort.value,
            "note_type": note_type.value,
        }
        return await self.post(uri, data)

    async def get_note_by_id(
        self,
        note_id: str,
        xsec_source: str,
        xsec_token: str,
    ) -> Dict:
        """
        è·å–ç¬”è®°è¯¦æƒ…API
        Args:
            note_id:ç¬”è®°ID
            xsec_source: æ¸ é“æ¥æº
            xsec_token: æœç´¢å…³é”®å­—ä¹‹åè¿”å›çš„æ¯”è¾ƒåˆ—è¡¨ä¸­è¿”å›çš„token

        Returns:

        """
        if xsec_source == "":
            xsec_source = "pc_search"

        data = {
            "source_note_id": note_id,
            "image_formats": ["jpg", "webp", "avif"],
            "extra": {"need_body_topic": 1},
            "xsec_source": xsec_source,
            "xsec_token": xsec_token,
        }
        uri = "/api/sns/web/v1/feed"
        res = await self.post(uri, data)
        if res and res.get("items"):
            res_dict: Dict = res["items"][0]["note_card"]
            return res_dict
        # çˆ¬å–é¢‘ç¹äº†å¯èƒ½ä¼šå‡ºç°æœ‰çš„ç¬”è®°èƒ½æœ‰ç»“æœæœ‰çš„æ²¡æœ‰
        utils.logger.error(
            f"[XiaoHongShuClient.get_note_by_id] get note id:{note_id} empty and res:{res}"
        )
        return dict()

    async def get_note_comments(
        self,
        note_id: str,
        xsec_token: str,
        cursor: str = "",
    ) -> Dict:
        """
        è·å–ä¸€çº§è¯„è®ºçš„API
        Args:
            note_id: ç¬”è®°ID
            xsec_token: éªŒè¯token
            cursor: åˆ†é¡µæ¸¸æ ‡

        Returns:

        """
        uri = "/api/sns/web/v2/comment/page"
        params = {
            "note_id": note_id,
            "cursor": cursor,
            "top_comment_id": "",
            "image_formats": "jpg,webp,avif",
            "xsec_token": xsec_token,
        }
        return await self.get(uri, params)

    async def get_note_sub_comments(
        self,
        note_id: str,
        root_comment_id: str,
        xsec_token: str,
        num: int = 10,
        cursor: str = "",
    ):
        """
        è·å–æŒ‡å®šçˆ¶è¯„è®ºä¸‹çš„å­è¯„è®ºçš„API
        Args:
            note_id: å­è¯„è®ºçš„å¸–å­ID
            root_comment_id: æ ¹è¯„è®ºID
            xsec_token: éªŒè¯token
            num: åˆ†é¡µæ•°é‡
            cursor: åˆ†é¡µæ¸¸æ ‡

        Returns:

        """
        uri = "/api/sns/web/v2/comment/sub/page"
        params = {
            "note_id": note_id,
            "root_comment_id": root_comment_id,
            "num": num,
            "cursor": cursor,
            "image_formats": "jpg,webp,avif",
            "top_comment_id": "",
            "xsec_token": xsec_token,
        }
        return await self.get(uri, params)

    async def get_note_all_comments(
        self,
        note_id: str,
        xsec_token: str,
        crawl_interval: float = 1.0,
        callback: Optional[Callable] = None,
        max_count: int = 10,
    ) -> List[Dict]:
        """
        è·å–æŒ‡å®šç¬”è®°ä¸‹çš„æ‰€æœ‰ä¸€çº§è¯„è®ºï¼Œè¯¥æ–¹æ³•ä¼šä¸€ç›´æŸ¥æ‰¾ä¸€ä¸ªå¸–å­ä¸‹çš„æ‰€æœ‰è¯„è®ºä¿¡æ¯
        Args:
            note_id: ç¬”è®°ID
            xsec_token: éªŒè¯token
            crawl_interval: çˆ¬å–ä¸€æ¬¡ç¬”è®°çš„å»¶è¿Ÿå•ä½ï¼ˆç§’ï¼‰
            callback: ä¸€æ¬¡ç¬”è®°çˆ¬å–ç»“æŸå
            max_count: ä¸€æ¬¡ç¬”è®°çˆ¬å–çš„æœ€å¤§è¯„è®ºæ•°é‡
        Returns:

        """
        import random

        result = []
        comments_has_more = True
        comments_cursor = ""

        # ğŸ”¥ æ·»åŠ è°ƒè¯•ä¿¡æ¯
        utils.logger.info(f"[XiaoHongShuClient.get_note_all_comments] å¼€å§‹è·å–è¯„è®º, note_id={note_id}, max_count={max_count}")

        page_num = 0
        while comments_has_more and len(result) < max_count:
            page_num += 1

            comments_res = await self.get_note_comments(
                note_id=note_id, xsec_token=xsec_token, cursor=comments_cursor
            )
            comments_has_more = comments_res.get("has_more", False)
            comments_cursor = comments_res.get("cursor", "")

            # ğŸ”¥ æ·»åŠ è°ƒè¯•ä¿¡æ¯
            utils.logger.info(f"[XiaoHongShuClient.get_note_all_comments] ç¬¬{page_num}é¡µ: å½“å‰å·²è·å–={len(result)}, has_more={comments_has_more}")

            if "comments" not in comments_res:
                utils.logger.info(
                    f"[XiaoHongShuClient.get_note_all_comments] No 'comments' key found in response: {comments_res}"
                )
                break
            comments = comments_res["comments"]

            # ğŸ”¥ æ·»åŠ è°ƒè¯•ä¿¡æ¯
            utils.logger.info(f"[XiaoHongShuClient.get_note_all_comments] æœ¬æ¬¡è·å–={len(comments)}æ¡è¯„è®º")

            if len(result) + len(comments) > max_count:
                comments = comments[: max_count - len(result)]
            if callback:
                await callback(note_id, comments)

            # ğŸ”¥ å¢åŠ éšæœºå»¶è¿Ÿ,æ¨¡æ‹ŸçœŸå®ç”¨æˆ·é˜…è¯»è¯„è®ºçš„æ—¶é—´
            delay = random.uniform(8, 12)
            utils.logger.info(f"[XiaoHongShuClient.get_note_all_comments] ç­‰å¾… {delay:.1f} ç§’åè·å–ä¸‹ä¸€é¡µ...")
            await asyncio.sleep(delay)

            result.extend(comments)
            sub_comments = await self.get_comments_all_sub_comments(
                comments=comments,
                xsec_token=xsec_token,
                crawl_interval=crawl_interval,
                callback=callback,
            )
            result.extend(sub_comments)

        # ğŸ”¥ æ·»åŠ è°ƒè¯•ä¿¡æ¯
        utils.logger.info(f"[XiaoHongShuClient.get_note_all_comments] å®Œæˆè·å–è¯„è®º, note_id={note_id}, æ€»è®¡={len(result)}æ¡")

        return result

    async def get_comments_all_sub_comments(
        self,
        comments: List[Dict],
        xsec_token: str,
        crawl_interval: float = 1.0,
        callback: Optional[Callable] = None,
    ) -> List[Dict]:
        """
        è·å–æŒ‡å®šä¸€çº§è¯„è®ºä¸‹çš„æ‰€æœ‰äºŒçº§è¯„è®º, è¯¥æ–¹æ³•ä¼šä¸€ç›´æŸ¥æ‰¾ä¸€çº§è¯„è®ºä¸‹çš„æ‰€æœ‰äºŒçº§è¯„è®ºä¿¡æ¯
        Args:
            comments: è¯„è®ºåˆ—è¡¨
            xsec_token: éªŒè¯token
            crawl_interval: çˆ¬å–ä¸€æ¬¡è¯„è®ºçš„å»¶è¿Ÿå•ä½ï¼ˆç§’ï¼‰
            callback: ä¸€æ¬¡è¯„è®ºçˆ¬å–ç»“æŸå

        Returns:

        """
        if not config.ENABLE_GET_SUB_COMMENTS:
            utils.logger.info(
                f"[XiaoHongShuCrawler.get_comments_all_sub_comments] Crawling sub_comment mode is not enabled"
            )
            return []

        result = []
        for comment in comments:
            note_id = comment.get("note_id")
            sub_comments = comment.get("sub_comments")
            if sub_comments and callback:
                await callback(note_id, sub_comments)

            sub_comment_has_more = comment.get("sub_comment_has_more")
            if not sub_comment_has_more:
                continue

            root_comment_id = comment.get("id")
            sub_comment_cursor = comment.get("sub_comment_cursor")

            while sub_comment_has_more:
                comments_res = await self.get_note_sub_comments(
                    note_id=note_id,
                    root_comment_id=root_comment_id,
                    xsec_token=xsec_token,
                    num=10,
                    cursor=sub_comment_cursor,
                )

                if comments_res is None:
                    utils.logger.info(
                        f"[XiaoHongShuClient.get_comments_all_sub_comments] No response found for note_id: {note_id}"
                    )
                    continue
                sub_comment_has_more = comments_res.get("has_more", False)
                sub_comment_cursor = comments_res.get("cursor", "")
                if "comments" not in comments_res:
                    utils.logger.info(
                        f"[XiaoHongShuClient.get_comments_all_sub_comments] No 'comments' key found in response: {comments_res}"
                    )
                    break
                comments = comments_res["comments"]
                if callback:
                    await callback(note_id, comments)
                await asyncio.sleep(crawl_interval)
                result.extend(comments)
        return result

    async def get_creator_info(
        self, user_id: str, xsec_token: str = "", xsec_source: str = ""
    ) -> Dict:
        """
        é€šè¿‡è§£æç½‘é¡µç‰ˆçš„ç”¨æˆ·ä¸»é¡µHTMLï¼Œè·å–ç”¨æˆ·ä¸ªäººç®€è¦ä¿¡æ¯
        PCç«¯ç”¨æˆ·ä¸»é¡µçš„ç½‘é¡µå­˜åœ¨window.__INITIAL_STATE__è¿™ä¸ªå˜é‡ä¸Šçš„ï¼Œè§£æå®ƒå³å¯

        Args:
            user_id: ç”¨æˆ·ID
            xsec_token: éªŒè¯token (å¯é€‰,å¦‚æœURLä¸­åŒ…å«æ­¤å‚æ•°åˆ™ä¼ å…¥)
            xsec_source: æ¸ é“æ¥æº (å¯é€‰,å¦‚æœURLä¸­åŒ…å«æ­¤å‚æ•°åˆ™ä¼ å…¥)

        Returns:
            Dict: åˆ›ä½œè€…ä¿¡æ¯
        """
        # æ„å»ºURI,å¦‚æœæœ‰xsecå‚æ•°åˆ™æ·»åŠ åˆ°URLä¸­
        uri = f"/user/profile/{user_id}"
        if xsec_token and xsec_source:
            uri = f"{uri}?xsec_token={xsec_token}&xsec_source={xsec_source}"

        html_content = await self.request(
            "GET", self._domain + uri, return_response=True, headers=self.headers
        )
        return self._extractor.extract_creator_info_from_html(html_content)

    async def get_notes_by_creator(
        self,
        creator: str,
        cursor: str,
        page_size: int = 30,
    ) -> Dict:
        """
        è·å–åšä¸»çš„ç¬”è®°
        Args:
            creator: åšä¸»ID
            cursor: ä¸Šä¸€é¡µæœ€åä¸€æ¡ç¬”è®°çš„ID
            page_size: åˆ†é¡µæ•°æ®é•¿åº¦

        Returns:

        """
        uri = "/api/sns/web/v1/user_posted"
        data = {
            "user_id": creator,
            "cursor": cursor,
            "num": page_size,
            "image_formats": "jpg,webp,avif",
        }
        return await self.get(uri, data)

    async def get_all_notes_by_creator(
        self,
        user_id: str,
        crawl_interval: float = 1.0,
        callback: Optional[Callable] = None,
    ) -> List[Dict]:
        """
        è·å–æŒ‡å®šç”¨æˆ·ä¸‹çš„æ‰€æœ‰å‘è¿‡çš„å¸–å­ï¼Œè¯¥æ–¹æ³•ä¼šä¸€ç›´æŸ¥æ‰¾ä¸€ä¸ªç”¨æˆ·ä¸‹çš„æ‰€æœ‰å¸–å­ä¿¡æ¯
        Args:
            user_id: ç”¨æˆ·ID
            crawl_interval: çˆ¬å–ä¸€æ¬¡çš„å»¶è¿Ÿå•ä½ï¼ˆç§’ï¼‰
            callback: ä¸€æ¬¡åˆ†é¡µçˆ¬å–ç»“æŸåçš„æ›´æ–°å›è°ƒå‡½æ•°

        Returns:

        """
        result = []
        notes_has_more = True
        notes_cursor = ""
        while notes_has_more and len(result) < config.CRAWLER_MAX_NOTES_COUNT:
            notes_res = await self.get_notes_by_creator(user_id, notes_cursor)
            if not notes_res:
                utils.logger.error(
                    f"[XiaoHongShuClient.get_notes_by_creator] The current creator may have been banned by xhs, so they cannot access the data."
                )
                break

            notes_has_more = notes_res.get("has_more", False)
            notes_cursor = notes_res.get("cursor", "")
            if "notes" not in notes_res:
                utils.logger.info(
                    f"[XiaoHongShuClient.get_all_notes_by_creator] No 'notes' key found in response: {notes_res}"
                )
                break

            notes = notes_res["notes"]
            utils.logger.info(
                f"[XiaoHongShuClient.get_all_notes_by_creator] got user_id:{user_id} notes len : {len(notes)}"
            )

            remaining = config.CRAWLER_MAX_NOTES_COUNT - len(result)
            if remaining <= 0:
                break

            notes_to_add = notes[:remaining]
            if callback:
                await callback(notes_to_add)

            result.extend(notes_to_add)
            await asyncio.sleep(crawl_interval)

        utils.logger.info(
            f"[XiaoHongShuClient.get_all_notes_by_creator] Finished getting notes for user {user_id}, total: {len(result)}"
        )
        return result

    async def get_note_short_url(self, note_id: str) -> Dict:
        """
        è·å–ç¬”è®°çš„çŸ­é“¾æ¥
        Args:
            note_id: ç¬”è®°ID

        Returns:

        """
        uri = f"/api/sns/web/short_url"
        data = {"original_url": f"{self._domain}/discovery/item/{note_id}"}
        return await self.post(uri, data=data, return_response=True)

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(1))
    async def get_note_by_id_from_html(
        self,
        note_id: str,
        xsec_source: str,
        xsec_token: str,
        enable_cookie: bool = False,
    ) -> Optional[Dict]:
        """
        é€šè¿‡è§£æç½‘é¡µç‰ˆçš„ç¬”è®°è¯¦æƒ…é¡µHTMLï¼Œè·å–ç¬”è®°è¯¦æƒ…, è¯¥æ¥å£å¯èƒ½ä¼šå‡ºç°å¤±è´¥çš„æƒ…å†µï¼Œè¿™é‡Œå°è¯•é‡è¯•3æ¬¡
        copy from https://github.com/ReaJason/xhs/blob/eb1c5a0213f6fbb592f0a2897ee552847c69ea2d/xhs/core.py#L217-L259
        thanks for ReaJason
        Args:
            note_id:
            xsec_source:
            xsec_token:
            enable_cookie:

        Returns:

        """
        url = (
            "https://www.xiaohongshu.com/explore/"
            + note_id
            + f"?xsec_token={xsec_token}&xsec_source={xsec_source}"
        )
        copy_headers = self.headers.copy()
        if not enable_cookie:
            del copy_headers["Cookie"]

        html = await self.request(
            method="GET", url=url, return_response=True, headers=copy_headers
        )

        return self._extractor.extract_note_detail_from_html(note_id, html)
